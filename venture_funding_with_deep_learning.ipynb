{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Venture Funding with Deep Learning\n",
    "\n",
    "You work as a risk management associate at Alphabet Soup, a venture capital firm. Alphabet Soup’s business team receives many funding applications from startups every day. This team has asked you to help them create a model that predicts whether applicants will be successful if funded by Alphabet Soup.\n",
    "\n",
    "The business team has given you a CSV containing more than 34,000 organizations that have received funding from Alphabet Soup over the years. With your knowledge of machine learning and neural networks, you decide to use the features in the provided dataset to create a binary classifier model that will predict whether an applicant will become a successful business. The CSV file contains a variety of information about these businesses, including whether or not they ultimately became successful.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "The steps for this challenge are broken out into the following sections:\n",
    "\n",
    "* Prepare the data for use on a neural network model.\n",
    "\n",
    "* Compile and evaluate a binary classification model using a neural network.\n",
    "\n",
    "* Optimize the neural network model.\n",
    "\n",
    "### Prepare the Data for Use on a Neural Network Model \n",
    "\n",
    "Using your knowledge of Pandas and scikit-learn’s `StandardScaler()`, preprocess the dataset so that you can use it to compile and evaluate the neural network model later.\n",
    "\n",
    "Open the starter code file, and complete the following data preparation steps:\n",
    "\n",
    "1. Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.   \n",
    "\n",
    "2. Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model.\n",
    " \n",
    "3. Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame.\n",
    "\n",
    "4. Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "> **Note** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. \n",
    "\n",
    "5. Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. \n",
    "\n",
    "6. Split the features and target sets into training and testing datasets.\n",
    "\n",
    "7. Use scikit-learn's `StandardScaler` to scale the features data.\n",
    "\n",
    "### Compile and Evaluate a Binary Classification Model Using a Neural Network\n",
    "\n",
    "Use your knowledge of TensorFlow to design a binary classification deep neural network model. This model should use the dataset’s features to predict whether an Alphabet Soup&ndash;funded startup will be successful based on the features in the dataset. Consider the number of inputs before determining the number of layers that your model will contain or the number of neurons on each layer. Then, compile and fit your model. Finally, evaluate your binary classification model to calculate the model’s loss and accuracy. \n",
    " \n",
    "To do so, complete the following steps:\n",
    "\n",
    "1. Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n",
    "\n",
    "2. Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n",
    "\n",
    "> **Hint** When fitting the model, start with a small number of epochs, such as 20, 50, or 100.\n",
    "\n",
    "3. Evaluate the model using the test data to determine the model’s loss and accuracy.\n",
    "\n",
    "4. Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. \n",
    "\n",
    "### Optimize the Neural Network Model\n",
    "\n",
    "Using your knowledge of TensorFlow and Keras, optimize your model to improve the model's accuracy. Even if you do not successfully achieve a better accuracy, you'll need to demonstrate at least two attempts to optimize the model. You can include these attempts in your existing notebook. Or, you can make copies of the starter notebook in the same folder, rename them, and code each model optimization in a new notebook. \n",
    "\n",
    "> **Note** You will not lose points if your model does not achieve a high accuracy, as long as you make at least two attempts to optimize the model.\n",
    "\n",
    "To do so, complete the following steps:\n",
    "\n",
    "1. Define at least three new deep neural network models (the original plus 2 optimization attempts). With each, try to improve on your first model’s predictive accuracy.\n",
    "\n",
    "> **Rewind** Recall that perfect accuracy has a value of 1, so accuracy improves as its value moves closer to 1. To optimize your model for a predictive accuracy as close to 1 as possible, you can use any or all of the following techniques:\n",
    ">\n",
    "> * Adjust the input data by dropping different features columns to ensure that no variables or outliers confuse the model.\n",
    ">\n",
    "> * Add more neurons (nodes) to a hidden layer.\n",
    ">\n",
    "> * Add more hidden layers.\n",
    ">\n",
    "> * Use different activation functions for the hidden layers.\n",
    ">\n",
    "> * Add to or reduce the number of epochs in the training regimen.\n",
    "\n",
    "2. After finishing your models, display the accuracy scores achieved by each model, and compare the results.\n",
    "\n",
    "3. Save each of your models as an HDF5 file.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame\n",
    "applicant_data_df = pd.read_csv(\n",
    "    Path('./Resources/applicants_data.csv')\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Review the data types associated with the columns\n",
    "applicant_data_df.dtypes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EIN                        int64\n",
       "NAME                      object\n",
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
    "applicant_data_df = applicant_data_df.drop(columns=['EIN', 'NAME'])\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0              T10       Independent          C1000    ProductDev   \n",
       "1               T3       Independent          C2000  Preservation   \n",
       "2               T5  CompanySponsored          C3000    ProductDev   \n",
       "3               T3  CompanySponsored          C2000  Preservation   \n",
       "4               T3       Independent          C1000     Heathcare   \n",
       "\n",
       "   ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  \\\n",
       "0   Association       1              0                      N     5000   \n",
       "1  Co-operative       1         1-9999                      N   108590   \n",
       "2   Association       1              0                      N     5000   \n",
       "3         Trust       1    10000-24999                      N     6692   \n",
       "4         Trust       1  100000-499999                      N   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables = list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"object\"].index)\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT',\n",
       " 'SPECIAL_CONSIDERATIONS']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables)\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  \\\n",
       "0                   0.0  ...                0.0                     0.0   \n",
       "1                   0.0  ...                1.0                     0.0   \n",
       "2                   0.0  ...                0.0                     0.0   \n",
       "3                   0.0  ...                0.0                     1.0   \n",
       "4                   0.0  ...                0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                       0.0                 0.0               0.0   \n",
       "1                       0.0                 0.0               0.0   \n",
       "2                       0.0                 0.0               0.0   \n",
       "3                       0.0                 0.0               0.0   \n",
       "4                       1.0                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       1.0                       0.0  \n",
       "1                       1.0                       0.0  \n",
       "2                       1.0                       0.0  \n",
       "3                       1.0                       0.0  \n",
       "4                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "> **Note** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "encoded_df = pd.concat([encoded_df, applicant_data_df.drop(columns=categorical_variables)], axis=1)\n",
    "\n",
    "# Review the Dataframe\n",
    "encoded_df.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                   0.0  ...                 0.0               0.0   \n",
       "1                   0.0  ...                 0.0               0.0   \n",
       "2                   0.0  ...                 0.0               0.0   \n",
       "3                   0.0  ...                 0.0               0.0   \n",
       "4                   0.0  ...                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  STATUS  ASK_AMT  \\\n",
       "0                       1.0                       0.0       1     5000   \n",
       "1                       1.0                       0.0       1   108590   \n",
       "2                       1.0                       0.0       1     5000   \n",
       "3                       1.0                       0.0       1     6692   \n",
       "4                       1.0                       0.0       1   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  \n",
       "\n",
       "[5 rows x 117 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = encoded_df['IS_SUCCESSFUL']\n",
    "\n",
    "# Display a sample of y\n",
    "y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: IS_SUCCESSFUL, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = encoded_df.drop(columns=['IS_SUCCESSFUL'])\n",
    "\n",
    "# Review the features DataFrame\n",
    "X.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  \\\n",
       "0                   0.0  ...                       0.0                 0.0   \n",
       "1                   0.0  ...                       0.0                 0.0   \n",
       "2                   0.0  ...                       0.0                 0.0   \n",
       "3                   0.0  ...                       0.0                 0.0   \n",
       "4                   0.0  ...                       1.0                 0.0   \n",
       "\n",
       "   INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  INCOME_AMT_50M+  \\\n",
       "0               0.0                     0.0              0.0   \n",
       "1               0.0                     0.0              0.0   \n",
       "2               0.0                     0.0              0.0   \n",
       "3               0.0                     0.0              0.0   \n",
       "4               0.0                     0.0              0.0   \n",
       "\n",
       "   INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \\\n",
       "0                0.0                       1.0                       0.0   \n",
       "1                0.0                       1.0                       0.0   \n",
       "2                0.0                       1.0                       0.0   \n",
       "3                0.0                       1.0                       0.0   \n",
       "4                0.0                       1.0                       0.0   \n",
       "\n",
       "   STATUS  ASK_AMT  \n",
       "0       1     5000  \n",
       "1       1   108590  \n",
       "2       1     5000  \n",
       "3       1     6692  \n",
       "4       1   142590  \n",
       "\n",
       "[5 rows x 116 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6: Split the features and target sets into training and testing datasets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 7: Use scikit-learn's `StandardScaler` to scale the features data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Compile and Evaluate a Binary Classification Model Using a Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "# Define global number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Define activation function\n",
    "act_function_hidden = 'relu'\n",
    "act_function_out = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 =  (number_input_features + number_output_neurons) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 =  (hidden_nodes_layer1 + number_output_neurons) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=act_function_hidden))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=act_function_hidden))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=number_output_neurons, activation=act_function_out))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_113 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_baseline = nn.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 986us/step - loss: 0.5781 - accuracy: 0.7180\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 951us/step - loss: 0.5549 - accuracy: 0.7280\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5505 - accuracy: 0.7307\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5489 - accuracy: 0.7313\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5466 - accuracy: 0.7331\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 927us/step - loss: 0.5452 - accuracy: 0.7327\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 943us/step - loss: 0.5443 - accuracy: 0.7344\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 954us/step - loss: 0.5435 - accuracy: 0.7344\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5432 - accuracy: 0.7352\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5421 - accuracy: 0.7356\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5418 - accuracy: 0.7345\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5413 - accuracy: 0.7352\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7358\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7352\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5395 - accuracy: 0.7369\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 963us/step - loss: 0.5392 - accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 947us/step - loss: 0.5383 - accuracy: 0.7369\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 969us/step - loss: 0.5385 - accuracy: 0.7371\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7365\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5376 - accuracy: 0.7372\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 943us/step - loss: 0.5374 - accuracy: 0.7374\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 982us/step - loss: 0.5368 - accuracy: 0.7371\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5366 - accuracy: 0.7386\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5361 - accuracy: 0.7381\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5363 - accuracy: 0.7387\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 955us/step - loss: 0.5361 - accuracy: 0.7371\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 950us/step - loss: 0.5359 - accuracy: 0.7378\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5355 - accuracy: 0.7392\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5354 - accuracy: 0.7378\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5351 - accuracy: 0.7388\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 987us/step - loss: 0.5349 - accuracy: 0.7383\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 981us/step - loss: 0.5347 - accuracy: 0.7385\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5342 - accuracy: 0.7382\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5348 - accuracy: 0.7399\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5341 - accuracy: 0.7398\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 990us/step - loss: 0.5339 - accuracy: 0.7387\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5337 - accuracy: 0.7398\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5337 - accuracy: 0.7399\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5331 - accuracy: 0.7391\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 903us/step - loss: 0.5334 - accuracy: 0.7402\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5329 - accuracy: 0.7398\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 997us/step - loss: 0.5332 - accuracy: 0.7397\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 959us/step - loss: 0.5328 - accuracy: 0.7385\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 900us/step - loss: 0.5331 - accuracy: 0.7409\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 917us/step - loss: 0.5325 - accuracy: 0.7407\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5325 - accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 908us/step - loss: 0.5323 - accuracy: 0.7406\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 937us/step - loss: 0.5323 - accuracy: 0.7407\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 936us/step - loss: 0.5315 - accuracy: 0.7411\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5324 - accuracy: 0.7403\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 928us/step - loss: 0.5320 - accuracy: 0.7405\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 916us/step - loss: 0.5320 - accuracy: 0.7401\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5317 - accuracy: 0.7411\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 907us/step - loss: 0.5317 - accuracy: 0.7408\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7410\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5318 - accuracy: 0.7404\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5315 - accuracy: 0.7411\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 935us/step - loss: 0.5318 - accuracy: 0.7399\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 912us/step - loss: 0.5309 - accuracy: 0.7410\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 963us/step - loss: 0.5311 - accuracy: 0.7414\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5310 - accuracy: 0.7424\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5309 - accuracy: 0.7416\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5313 - accuracy: 0.7420\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 922us/step - loss: 0.5308 - accuracy: 0.7417\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5309 - accuracy: 0.7416\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 942us/step - loss: 0.5305 - accuracy: 0.7411\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5307 - accuracy: 0.7411\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7416\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7418\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7420\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5304 - accuracy: 0.7408\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7413\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 950us/step - loss: 0.5304 - accuracy: 0.7410\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7420\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5297 - accuracy: 0.7419\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7423\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7425\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5300 - accuracy: 0.7418\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 951us/step - loss: 0.5299 - accuracy: 0.7411\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 926us/step - loss: 0.5297 - accuracy: 0.7420\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 934us/step - loss: 0.5297 - accuracy: 0.7425\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5294 - accuracy: 0.7417\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 932us/step - loss: 0.5292 - accuracy: 0.7419\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 933us/step - loss: 0.5294 - accuracy: 0.7420\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 915us/step - loss: 0.5293 - accuracy: 0.7425\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5292 - accuracy: 0.7424\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 910us/step - loss: 0.5295 - accuracy: 0.7427\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 907us/step - loss: 0.5296 - accuracy: 0.7423\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5300 - accuracy: 0.7425\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7433\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5292 - accuracy: 0.7425\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 933us/step - loss: 0.5290 - accuracy: 0.7427\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 924us/step - loss: 0.5293 - accuracy: 0.7423\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5294 - accuracy: 0.7425\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5287 - accuracy: 0.7421\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5289 - accuracy: 0.7419\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5286 - accuracy: 0.7427\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5285 - accuracy: 0.7427\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_baseline, model_accuracy_baseline = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_baseline}, Accuracy: {model_accuracy_baseline}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "268/268 - 0s - loss: 0.5663 - accuracy: 0.7313\n",
      "Loss: 0.5663371682167053, Accuracy: 0.7313119769096375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"./Resources/AlphabetSoup.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Optimize the neural network model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Define at least three new deep neural network models (resulting in the original plus 3 optimization attempts). With each, try to improve on your first model’s predictive accuracy.\n",
    "\n",
    "> **Rewind** Recall that perfect accuracy has a value of 1, so accuracy improves as its value moves closer to 1. To optimize your model for a predictive accuracy as close to 1 as possible, you can use any or all of the following techniques:\n",
    ">\n",
    "> * Adjust the input data by dropping different features columns to ensure that no variables or outliers confuse the model.\n",
    ">\n",
    "> * Add more neurons (nodes) to a hidden layer.\n",
    ">\n",
    "> * Add more hidden layers.\n",
    ">\n",
    "> * Use different activation functions for the hidden layers.\n",
    ">\n",
    "> * Add to or reduce the number of epochs in the training regimen.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alternative Model 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A1 = 'relu'\n",
    "act_function_out_A1 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A1 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A1}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A1 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A1 =  (number_input_features_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A1 =  (hidden_nodes_layer1_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer2_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer3_A1 = (hidden_nodes_layer2_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 3: {hidden_nodes_layer3_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer4_A1 = (hidden_nodes_layer3_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 4: {hidden_nodes_layer4_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer5_A1 = (hidden_nodes_layer4_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 5: {hidden_nodes_layer5_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer6_A1 = (hidden_nodes_layer5_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 6: {hidden_nodes_layer6_A1}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A1 = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer1_A1, input_dim=number_input_features_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer2_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer3_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer4_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer5_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer6_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Output layer\n",
    "nn_A1.add(Dense(units=number_output_neurons_A1, activation=act_function_out_A1))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A1.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 29\n",
      "Hidden layer 3: 15\n",
      "Hidden layer 4: 8\n",
      "Hidden layer 5: 4\n",
      "Hidden layer 6: 2\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9,124\n",
      "Trainable params: 9,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A1 = nn_A1.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.6109 - accuracy: 0.7156\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5795 - accuracy: 0.7299\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 982us/step - loss: 0.5676 - accuracy: 0.7307\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 932us/step - loss: 0.5607 - accuracy: 0.7310\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 942us/step - loss: 0.5567 - accuracy: 0.7339\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 983us/step - loss: 0.5552 - accuracy: 0.7330\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5533 - accuracy: 0.7335\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5513 - accuracy: 0.7352\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5494 - accuracy: 0.7347\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5482 - accuracy: 0.7350\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5480 - accuracy: 0.7346\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 981us/step - loss: 0.5476 - accuracy: 0.7331\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 946us/step - loss: 0.5465 - accuracy: 0.7357\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7357\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5452 - accuracy: 0.7363\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 967us/step - loss: 0.5447 - accuracy: 0.7369\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 993us/step - loss: 0.5446 - accuracy: 0.7366\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.7369\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5436 - accuracy: 0.7363\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5423 - accuracy: 0.7374\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 955us/step - loss: 0.5429 - accuracy: 0.7373\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5423 - accuracy: 0.7371\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5415 - accuracy: 0.7387\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 972us/step - loss: 0.5414 - accuracy: 0.7376\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7365\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 958us/step - loss: 0.5406 - accuracy: 0.7388\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5413 - accuracy: 0.7374\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 999us/step - loss: 0.5405 - accuracy: 0.7388\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5401 - accuracy: 0.7378\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 956us/step - loss: 0.5399 - accuracy: 0.7378\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5396 - accuracy: 0.7383\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5394 - accuracy: 0.7390\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 957us/step - loss: 0.5396 - accuracy: 0.7384\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 941us/step - loss: 0.5387 - accuracy: 0.7389\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5390 - accuracy: 0.7374\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5387 - accuracy: 0.7389\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5383 - accuracy: 0.7403\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5376 - accuracy: 0.7396\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5383 - accuracy: 0.7395\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5381 - accuracy: 0.7382\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5373 - accuracy: 0.7398\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5373 - accuracy: 0.7403\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5377 - accuracy: 0.7392\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7391\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5371 - accuracy: 0.7406\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5374 - accuracy: 0.7399\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5367 - accuracy: 0.7399\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7403\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 997us/step - loss: 0.5367 - accuracy: 0.7411\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5367 - accuracy: 0.7406\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7397\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5364 - accuracy: 0.7410\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7392\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 987us/step - loss: 0.5361 - accuracy: 0.7408\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5358 - accuracy: 0.7389\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5350 - accuracy: 0.7417\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5343 - accuracy: 0.7415\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5352 - accuracy: 0.7405\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5351 - accuracy: 0.7405\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5343 - accuracy: 0.7415\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5340 - accuracy: 0.7412\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5337 - accuracy: 0.7420\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7408\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 968us/step - loss: 0.5332 - accuracy: 0.7402\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7408\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5331 - accuracy: 0.7409\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5339 - accuracy: 0.7414\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7401\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7401\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 988us/step - loss: 0.5331 - accuracy: 0.7412\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5330 - accuracy: 0.7411\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7421\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 983us/step - loss: 0.5326 - accuracy: 0.7407\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5326 - accuracy: 0.7418\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7418\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7418\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5326 - accuracy: 0.7417\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5320 - accuracy: 0.7416\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5327 - accuracy: 0.7414\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5317 - accuracy: 0.7425\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7411\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7404\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5324 - accuracy: 0.7430\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 971us/step - loss: 0.5321 - accuracy: 0.7418\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7414\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7416\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7410\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7415\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7418\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7409\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7428\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5308 - accuracy: 0.7428\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5308 - accuracy: 0.7427\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 994us/step - loss: 0.5308 - accuracy: 0.7423\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7421\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7424\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7409\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7423\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7413\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7432\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A2 = 'selu'\n",
    "act_function_out_A2 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A2 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A2}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A2 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A2 =  (number_input_features_A2 + number_output_neurons_A2) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A2}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A2 =  (hidden_nodes_layer1_A2 + number_output_neurons_A2) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1_A2}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A2.add(Dense(units=hidden_nodes_layer1_A2, input_dim=number_input_features_A2, activation=act_function_hidden_A2))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A2.add(Dense(units=hidden_nodes_layer2_A2, activation=act_function_hidden_A2))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A2.add(Dense(units=number_output_neurons_A2, activation=act_function_out_A2))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A2.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A2 = nn_A2.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 890us/step - loss: 0.5887 - accuracy: 0.7205\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5610 - accuracy: 0.7281\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5560 - accuracy: 0.7301\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7322\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5520 - accuracy: 0.7313\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7299\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7305\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 944us/step - loss: 0.5481 - accuracy: 0.7315\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5470 - accuracy: 0.7330\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 934us/step - loss: 0.5468 - accuracy: 0.7329\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5463 - accuracy: 0.7331\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 788us/step - loss: 0.5458 - accuracy: 0.7338\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7341\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 763us/step - loss: 0.5449 - accuracy: 0.7325\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 938us/step - loss: 0.5445 - accuracy: 0.7335\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 711us/step - loss: 0.5434 - accuracy: 0.7343\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 716us/step - loss: 0.5437 - accuracy: 0.7342\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 719us/step - loss: 0.5437 - accuracy: 0.7339\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.7351\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5430 - accuracy: 0.7350\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7362\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5424 - accuracy: 0.7337\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5421 - accuracy: 0.7348\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7336\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5412 - accuracy: 0.7358\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5412 - accuracy: 0.7353\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 994us/step - loss: 0.5413 - accuracy: 0.7362\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5413 - accuracy: 0.7351\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 845us/step - loss: 0.5409 - accuracy: 0.7358\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 809us/step - loss: 0.5404 - accuracy: 0.7362\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 826us/step - loss: 0.5401 - accuracy: 0.7358\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 862us/step - loss: 0.5409 - accuracy: 0.7345\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 858us/step - loss: 0.5400 - accuracy: 0.7353\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 839us/step - loss: 0.5398 - accuracy: 0.7374\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 836us/step - loss: 0.5399 - accuracy: 0.7362\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5395 - accuracy: 0.7364\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 848us/step - loss: 0.5397 - accuracy: 0.7374\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 840us/step - loss: 0.5392 - accuracy: 0.7374\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 852us/step - loss: 0.5395 - accuracy: 0.7367\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 848us/step - loss: 0.5388 - accuracy: 0.7356\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 853us/step - loss: 0.5394 - accuracy: 0.7364\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 843us/step - loss: 0.5387 - accuracy: 0.7371\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 827us/step - loss: 0.5386 - accuracy: 0.7376\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 826us/step - loss: 0.5393 - accuracy: 0.7383\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 881us/step - loss: 0.5381 - accuracy: 0.7375\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5385 - accuracy: 0.7365\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 834us/step - loss: 0.5388 - accuracy: 0.7364\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 861us/step - loss: 0.5380 - accuracy: 0.7377\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 842us/step - loss: 0.5382 - accuracy: 0.7349\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5379 - accuracy: 0.7376\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 842us/step - loss: 0.5380 - accuracy: 0.7381\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 873us/step - loss: 0.5379 - accuracy: 0.7383\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 843us/step - loss: 0.5373 - accuracy: 0.7373\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 970us/step - loss: 0.5370 - accuracy: 0.7367\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 844us/step - loss: 0.5371 - accuracy: 0.7383\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 872us/step - loss: 0.5376 - accuracy: 0.7373\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1000us/step - loss: 0.5371 - accuracy: 0.7371\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 863us/step - loss: 0.5367 - accuracy: 0.7394\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 846us/step - loss: 0.5368 - accuracy: 0.7388\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5368 - accuracy: 0.7389\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5368 - accuracy: 0.7383\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5362 - accuracy: 0.7381\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 859us/step - loss: 0.5361 - accuracy: 0.7385\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5363 - accuracy: 0.7378\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 865us/step - loss: 0.5363 - accuracy: 0.7377\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5366 - accuracy: 0.7383\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 892us/step - loss: 0.5363 - accuracy: 0.7376\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 896us/step - loss: 0.5369 - accuracy: 0.7372\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5360 - accuracy: 0.7380\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 883us/step - loss: 0.5363 - accuracy: 0.7377\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5361 - accuracy: 0.7380\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 873us/step - loss: 0.5362 - accuracy: 0.7388\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 886us/step - loss: 0.5360 - accuracy: 0.7395\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 939us/step - loss: 0.5362 - accuracy: 0.7377\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 887us/step - loss: 0.5362 - accuracy: 0.7391\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5359 - accuracy: 0.7383\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5353 - accuracy: 0.7389\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 893us/step - loss: 0.5358 - accuracy: 0.7374\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 944us/step - loss: 0.5356 - accuracy: 0.7384\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5356 - accuracy: 0.7386\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 971us/step - loss: 0.5355 - accuracy: 0.7390\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 882us/step - loss: 0.5350 - accuracy: 0.7394\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5355 - accuracy: 0.7394\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 881us/step - loss: 0.5349 - accuracy: 0.7397\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 851us/step - loss: 0.5349 - accuracy: 0.7397\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5350 - accuracy: 0.7394\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 900us/step - loss: 0.5348 - accuracy: 0.7410\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 922us/step - loss: 0.5353 - accuracy: 0.7384\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 957us/step - loss: 0.5347 - accuracy: 0.7382\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 864us/step - loss: 0.5352 - accuracy: 0.7383\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5348 - accuracy: 0.7380\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5351 - accuracy: 0.7399\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 874us/step - loss: 0.5349 - accuracy: 0.7376\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5346 - accuracy: 0.7397\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7387\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7394\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5347 - accuracy: 0.7395\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5349 - accuracy: 0.7389\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7389\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A3 = 'gelu'\n",
    "act_function_out_A3 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A3 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A3}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A3 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A3 =  (number_input_features_A3 + number_output_neurons_A3) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A3}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A3 =  (hidden_nodes_layer1_A3 + number_output_neurons_A3) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1_A3}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A3 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A3.add(Dense(units=hidden_nodes_layer1_A3, input_dim=number_input_features_A3, activation=act_function_hidden_A3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A3.add(Dense(units=hidden_nodes_layer2_A3, activation=act_function_hidden_A3))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A3.add(Dense(units=number_output_neurons_A3, activation=act_function_out_A3))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A3.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_107 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A3 = nn_A3.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5767 - accuracy: 0.7180\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5531 - accuracy: 0.7304\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7318\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 947us/step - loss: 0.5482 - accuracy: 0.7325\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 958us/step - loss: 0.5460 - accuracy: 0.7338\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5451 - accuracy: 0.7334\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7336\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5429 - accuracy: 0.7361\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5429 - accuracy: 0.7355\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7355\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5406 - accuracy: 0.7365\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.7377\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 992us/step - loss: 0.5406 - accuracy: 0.7370\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5401 - accuracy: 0.7372\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5394 - accuracy: 0.7371\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5391 - accuracy: 0.7375\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5380 - accuracy: 0.7371\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5378 - accuracy: 0.7379\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7397\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7383\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7380\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7387\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5360 - accuracy: 0.7391\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5358 - accuracy: 0.7390\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5356 - accuracy: 0.7383\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5352 - accuracy: 0.7381\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5351 - accuracy: 0.7400\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7392\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7394\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7398\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5342 - accuracy: 0.7402\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5337 - accuracy: 0.7401\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7404\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7405\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7407\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5333 - accuracy: 0.7395\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7401\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7416\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7414\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7417\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5325 - accuracy: 0.7410\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7417\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7413\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5321 - accuracy: 0.7419\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7405\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7414\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7419\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7407\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7416\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7415\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7419\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7414\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7407\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7411\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7414\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7414\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5308 - accuracy: 0.7418\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7417\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7412\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7414\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5304 - accuracy: 0.7416\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7421\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7418\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7425\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7423\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5296 - accuracy: 0.7425\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7417\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7421\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7418\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7420\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7427\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7433\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7422\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7428\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5289 - accuracy: 0.7437\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5288 - accuracy: 0.7425\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7420\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7430\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7429\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7428\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5290 - accuracy: 0.7422\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5285 - accuracy: 0.7429\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7426\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7434\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7423\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7419\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5281 - accuracy: 0.7428\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.7423\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5288 - accuracy: 0.7423\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5277 - accuracy: 0.7427\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5277 - accuracy: 0.7430\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5280 - accuracy: 0.7422\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5279 - accuracy: 0.7427\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5278 - accuracy: 0.7429\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5278 - accuracy: 0.7428\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5274 - accuracy: 0.7439\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5275 - accuracy: 0.7430\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A4 = 'gelu'\n",
    "act_function_out_A4 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A4 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A4}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A4 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A4 =  (number_input_features_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A4 =  (hidden_nodes_layer1_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer2_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer3_A4 = (hidden_nodes_layer2_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 3: {hidden_nodes_layer3_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer4_A4 = (hidden_nodes_layer3_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 4: {hidden_nodes_layer4_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer5_A4 = (hidden_nodes_layer4_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 5: {hidden_nodes_layer5_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer6_A4 = (hidden_nodes_layer5_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 6: {hidden_nodes_layer6_A4}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A4 = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer1_A4, input_dim=number_input_features_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer2_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer3_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer4_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer5_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer6_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Output layer\n",
    "nn_A4.add(Dense(units=number_output_neurons_A4, activation=act_function_out_A4))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A4.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 29\n",
      "Hidden layer 3: 15\n",
      "Hidden layer 4: 8\n",
      "Hidden layer 5: 4\n",
      "Hidden layer 6: 2\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9,124\n",
      "Trainable params: 9,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A4 = nn_A4.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 2s 1ms/step - loss: 0.5771 - accuracy: 0.7113\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5515 - accuracy: 0.7319\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5483 - accuracy: 0.7334\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7350\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7349\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.7347\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5424 - accuracy: 0.7366\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7364\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5411 - accuracy: 0.7362\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7369\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7372\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7381\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5382 - accuracy: 0.7383\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.7375\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7379\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7379\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5368 - accuracy: 0.7377\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7378\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7388\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5359 - accuracy: 0.7385\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5350 - accuracy: 0.7387\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7392\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7398\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7390\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5344 - accuracy: 0.7392\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7391\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7396\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7389\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5333 - accuracy: 0.7407\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7386\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5343 - accuracy: 0.7394\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5354 - accuracy: 0.7380\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7399\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5329 - accuracy: 0.7381\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5335 - accuracy: 0.7384\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7391\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5336 - accuracy: 0.7392\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7394\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7388\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7395\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7389\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5327 - accuracy: 0.7388\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7386\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7396\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7388\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7396\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7402\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7391\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5315 - accuracy: 0.7397\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7403\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7374\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7396\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5315 - accuracy: 0.7402\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5323 - accuracy: 0.7395\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7405\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5314 - accuracy: 0.7395\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7392\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5309 - accuracy: 0.7408\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7387\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7395\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7401\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7404\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7406\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7405\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7399\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7408\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7400\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7389\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7399\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7404\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7397\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7407\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7415\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7403\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7414\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7399\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5304 - accuracy: 0.7400\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7399\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7400\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5296 - accuracy: 0.7402\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7409\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5317 - accuracy: 0.7382\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7411\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7402\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7406\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7417\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7409\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7408\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7393\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7403\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7405\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7409\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7418\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7413\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.7411\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7410\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5298 - accuracy: 0.7406\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: After finishing your models, display the accuracy scores achieved by each model, and compare the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "# Create a DataFrame using the model history for loss\n",
    "models_plot_loss = pd.DataFrame(\n",
    "    {'loss_baseline' : fit_model_baseline.history['loss'],\n",
    "    'loss_A1' : fit_model_A1.history['loss'],\n",
    "    'loss_A2' : fit_model_A2.history['loss'],\n",
    "    'loss_A3' : fit_model_A3.history['loss'],\n",
    "    'loss_A4' : fit_model_A4.history['loss']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Vizualize the model plot where the y-axis displays the loss metric\n",
    "models_plot_loss.plot(title='Model Loss', xlabel='epochs', ylabel='loss')\n",
    "\n",
    "# Create a DataFrame using the model history for accuracy\n",
    "models_plot_accuracy = pd.DataFrame(\n",
    "    {'accuracy_baseline' : fit_model_baseline.history['accuracy'],\n",
    "    'accuracy_A1' : fit_model_A1.history['accuracy'],\n",
    "    'accuracy_A2' : fit_model_A2.history['accuracy'],\n",
    "    'accuracy_A3' : fit_model_A3.history['accuracy'],\n",
    "    'accuracy_A4' : fit_model_A4.history['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Vizualize the model plot where the y-axis displays the accuracy metric\n",
    "models_plot_accuracy.plot(title='Model Accuracy', xlabel='epochs', ylabel='accuracy')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Model Accuracy'}, xlabel='epochs', ylabel='accuracy'>"
      ]
     },
     "metadata": {},
     "execution_count": 194
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-15T23:20:09.568835</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m352e26f92d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.849535\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(120.487035 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.337138\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(181.974638 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.824742\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(243.462242 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.312345\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(304.949845 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"372.799948\" xlink:href=\"#m352e26f92d\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(363.256198 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <g transform=\"translate(199.710938 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m06645577e7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m06645577e7\" y=\"200.046906\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.54 -->\n      <g transform=\"translate(20.878125 203.846125)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m06645577e7\" y=\"152.729006\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.56 -->\n      <g transform=\"translate(20.878125 156.528225)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m06645577e7\" y=\"105.411106\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.58 -->\n      <g transform=\"translate(20.878125 109.210325)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m06645577e7\" y=\"58.093206\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.60 -->\n      <g transform=\"translate(20.878125 61.892425)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 140.695937)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p6dd6decf8e)\" d=\"M 65.361932 109.800059 \nL 68.436312 164.909741 \nL 71.510692 175.291934 \nL 74.585072 179.026099 \nL 77.659452 184.44078 \nL 80.733833 187.738916 \nL 83.808213 189.8941 \nL 86.882593 191.764003 \nL 89.956973 192.462749 \nL 93.031353 195.123341 \nL 96.105733 195.819126 \nL 99.180114 196.860123 \nL 102.254494 199.633954 \nL 105.328874 200.107493 \nL 108.403254 201.115774 \nL 111.477634 202.025061 \nL 114.552014 204.130464 \nL 117.626395 203.504202 \nL 120.700775 203.81557 \nL 123.775155 205.622015 \nL 126.849535 206.217253 \nL 129.923915 207.659871 \nL 132.998295 208.055709 \nL 136.072676 209.385371 \nL 139.147056 208.775749 \nL 142.221436 209.296389 \nL 145.295816 209.6691 \nL 148.370196 210.587976 \nL 151.444576 210.806554 \nL 154.518957 210.974225 \nL 157.593337 211.686931 \nL 160.667717 212.16033 \nL 163.742097 212.490736 \nL 166.816477 213.748196 \nL 169.890857 212.297823 \nL 172.965238 213.936456 \nL 176.039618 214.483466 \nL 179.113998 214.883394 \nL 182.188378 214.978863 \nL 185.262758 216.422609 \nL 188.337138 215.559718 \nL 191.411519 216.823101 \nL 194.485899 216.145226 \nL 197.560279 217.008117 \nL 200.634659 216.375791 \nL 203.709039 217.730413 \nL 206.783419 216.988375 \nL 209.8578 217.69403 \nL 212.93218 218.198453 \nL 216.00656 218.291807 \nL 219.08094 220.246744 \nL 222.15532 218.128226 \nL 225.2297 218.9749 \nL 228.304081 218.956567 \nL 231.378461 219.587483 \nL 234.452841 219.671953 \nL 237.527221 219.230707 \nL 240.601601 219.534461 \nL 243.675981 220.158467 \nL 246.750362 219.41812 \nL 249.824742 221.514499 \nL 252.899122 221.036588 \nL 255.973502 221.272794 \nL 259.047882 221.514076 \nL 262.122262 220.649351 \nL 265.196643 221.894261 \nL 268.271023 221.563009 \nL 271.345403 222.491051 \nL 274.419783 221.952643 \nL 277.494163 222.34834 \nL 280.568543 222.131031 \nL 283.642924 222.084354 \nL 286.717304 222.866865 \nL 289.791684 223.27751 \nL 292.866064 222.663375 \nL 295.940444 223.480859 \nL 299.014824 224.522279 \nL 302.089205 223.325174 \nL 305.163585 225.039675 \nL 308.237965 223.688579 \nL 311.312345 223.982179 \nL 314.386725 224.397337 \nL 317.461105 224.334019 \nL 320.535486 225.063225 \nL 323.609866 225.489242 \nL 326.684246 225.151503 \nL 329.758626 225.258959 \nL 332.833006 225.551149 \nL 335.907386 224.935604 \nL 338.981767 224.754254 \nL 342.056147 223.792791 \nL 345.130527 227.06625 \nL 348.204907 225.643093 \nL 351.279287 226.08039 \nL 354.353667 225.359082 \nL 357.428048 225.11794 \nL 360.502428 226.860786 \nL 363.576808 226.252433 \nL 366.651188 226.945538 \nL 369.725568 227.298225 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p6dd6decf8e)\" d=\"M 65.361932 32.201761 \nL 68.436312 106.500794 \nL 71.510692 134.823762 \nL 74.585072 150.966553 \nL 77.659452 160.600926 \nL 80.733833 164.149652 \nL 83.808213 168.513746 \nL 86.882593 173.195837 \nL 89.956973 177.915439 \nL 93.031353 180.743138 \nL 96.105733 181.007406 \nL 99.180114 181.977895 \nL 102.254494 184.679947 \nL 105.328874 185.886077 \nL 108.403254 187.707751 \nL 111.477634 188.825322 \nL 114.552014 189.227083 \nL 117.626395 191.461095 \nL 120.700775 191.561923 \nL 123.775155 194.651212 \nL 126.849535 193.127932 \nL 129.923915 194.608483 \nL 132.998295 196.433261 \nL 136.072676 196.762538 \nL 139.147056 197.806638 \nL 142.221436 198.657543 \nL 145.295816 196.959682 \nL 148.370196 198.85934 \nL 151.444576 199.732526 \nL 154.518957 200.207193 \nL 157.593337 200.93104 \nL 160.667717 201.560123 \nL 163.742097 200.901991 \nL 166.816477 203.151092 \nL 169.890857 202.344749 \nL 172.965238 203.093979 \nL 176.039618 204.138079 \nL 179.113998 205.72651 \nL 182.188378 203.959127 \nL 185.262758 204.451563 \nL 188.337138 206.489701 \nL 191.411519 206.359259 \nL 194.485899 205.415846 \nL 197.560279 206.890052 \nL 200.634659 206.875668 \nL 203.709039 206.301159 \nL 206.783419 207.823311 \nL 209.8578 208.261596 \nL 212.93218 207.860681 \nL 216.00656 207.753366 \nL 219.08094 207.275737 \nL 222.15532 208.573388 \nL 225.2297 208.89082 \nL 228.304081 209.201765 \nL 231.378461 210.008108 \nL 234.452841 211.760543 \nL 237.527221 213.585743 \nL 240.601601 211.354974 \nL 243.675981 211.583283 \nL 246.750362 213.483364 \nL 249.824742 214.215531 \nL 252.899122 214.984504 \nL 255.973502 213.064257 \nL 259.047882 216.050603 \nL 262.122262 215.679724 \nL 265.196643 216.475068 \nL 268.271023 214.410277 \nL 271.345403 216.18739 \nL 274.419783 216.267207 \nL 277.494163 216.455748 \nL 280.568543 216.490862 \nL 283.642924 217.539897 \nL 286.717304 217.460222 \nL 289.791684 217.634662 \nL 292.866064 218.547755 \nL 295.940444 217.559499 \nL 299.014824 217.503938 \nL 302.089205 219.073472 \nL 305.163585 217.385482 \nL 308.237965 219.678581 \nL 311.312345 218.392635 \nL 314.386725 219.809164 \nL 317.461105 218.036987 \nL 320.535486 218.723464 \nL 323.609866 220.461515 \nL 326.684246 219.807895 \nL 329.758626 221.135865 \nL 332.833006 220.631865 \nL 335.907386 220.585611 \nL 338.981767 221.215258 \nL 342.056147 221.023614 \nL 345.130527 221.731244 \nL 348.204907 221.709386 \nL 351.279287 221.822342 \nL 354.353667 223.228154 \nL 357.428048 220.304703 \nL 360.502428 221.121622 \nL 363.576808 221.992269 \nL 366.651188 223.068662 \nL 369.725568 223.79547 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p6dd6decf8e)\" d=\"M 65.361932 84.942899 \nL 68.436312 150.248629 \nL 71.510692 162.180895 \nL 74.585072 171.838254 \nL 77.659452 171.748848 \nL 80.733833 175.338188 \nL 83.808213 178.210449 \nL 86.882593 180.884721 \nL 89.956973 183.556736 \nL 93.031353 183.929729 \nL 96.105733 185.210881 \nL 99.180114 186.369064 \nL 102.254494 188.749313 \nL 105.328874 188.370256 \nL 108.403254 189.434662 \nL 111.477634 192.010503 \nL 114.552014 191.318385 \nL 117.626395 191.361677 \nL 120.700775 192.457531 \nL 123.775155 193.026681 \nL 126.849535 195.496476 \nL 129.923915 194.265104 \nL 132.998295 195.070036 \nL 136.072676 195.652301 \nL 139.147056 197.159787 \nL 142.221436 197.095765 \nL 145.295816 196.901865 \nL 148.370196 196.923017 \nL 151.444576 197.967681 \nL 154.518957 199.112327 \nL 157.593337 199.737038 \nL 160.667717 197.877711 \nL 163.742097 199.993409 \nL 166.816477 200.564675 \nL 169.890857 200.190694 \nL 172.965238 201.16358 \nL 176.039618 200.833315 \nL 179.113998 201.853441 \nL 182.188378 201.145529 \nL 185.262758 202.963396 \nL 188.337138 201.517112 \nL 191.411519 203.156028 \nL 194.485899 203.468242 \nL 197.560279 201.662784 \nL 200.634659 204.642361 \nL 203.709039 203.512522 \nL 206.783419 202.836198 \nL 209.8578 204.87772 \nL 212.93218 204.190115 \nL 216.00656 204.912693 \nL 219.08094 204.684102 \nL 222.15532 204.926936 \nL 225.2297 206.367438 \nL 228.304081 207.139795 \nL 231.378461 206.808543 \nL 234.452841 205.684909 \nL 237.527221 206.800082 \nL 240.601601 207.936972 \nL 243.675981 207.62927 \nL 246.750362 207.583862 \nL 249.824742 207.540146 \nL 252.899122 209.011955 \nL 255.973502 209.283556 \nL 259.047882 208.850066 \nL 262.122262 208.773493 \nL 265.196643 208.15823 \nL 268.271023 208.774339 \nL 271.345403 207.385167 \nL 274.419783 209.467021 \nL 277.494163 208.856976 \nL 280.568543 209.238571 \nL 283.642924 209.151845 \nL 286.717304 209.618898 \nL 289.791684 209.029441 \nL 292.866064 209.008147 \nL 295.940444 209.824079 \nL 299.014824 211.126102 \nL 302.089205 210.091309 \nL 305.163585 210.550606 \nL 308.237965 210.450765 \nL 311.312345 210.612795 \nL 314.386725 211.975173 \nL 317.461105 210.636063 \nL 320.535486 210.63014 \nL 323.609866 212.015504 \nL 326.684246 212.008453 \nL 329.758626 211.761107 \nL 332.833006 212.261863 \nL 335.907386 211.231019 \nL 338.981767 212.547143 \nL 342.056147 211.488801 \nL 345.130527 212.392446 \nL 348.204907 211.698636 \nL 351.279287 212.030452 \nL 354.353667 212.732159 \nL 357.428048 212.370588 \nL 360.502428 212.463237 \nL 363.576808 212.654035 \nL 366.651188 211.999569 \nL 369.725568 212.691687 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p6dd6decf8e)\" d=\"M 65.361932 113.186473 \nL 68.436312 169.019156 \nL 71.510692 175.426606 \nL 74.585072 180.680667 \nL 77.659452 185.869719 \nL 80.733833 187.910677 \nL 83.808213 188.648062 \nL 86.882593 193.167276 \nL 89.956973 193.109177 \nL 93.031353 195.651878 \nL 96.105733 198.612276 \nL 99.180114 199.423413 \nL 102.254494 198.704925 \nL 105.328874 199.857891 \nL 108.403254 201.533048 \nL 111.477634 202.232216 \nL 114.552014 204.77661 \nL 117.626395 205.295981 \nL 120.700775 206.878488 \nL 123.775155 206.748611 \nL 126.849535 207.829093 \nL 129.923915 207.965176 \nL 132.998295 209.511583 \nL 136.072676 210.061695 \nL 139.147056 210.432009 \nL 142.221436 211.354692 \nL 145.295816 211.523632 \nL 148.370196 212.400625 \nL 151.444576 212.830731 \nL 154.518957 212.7433 \nL 157.593337 213.718724 \nL 160.667717 215.047398 \nL 163.742097 216.180199 \nL 166.816477 215.586229 \nL 169.890857 215.673238 \nL 172.965238 215.898021 \nL 176.039618 217.467414 \nL 179.113998 216.050744 \nL 182.188378 217.140392 \nL 185.262758 218.423095 \nL 188.337138 217.802473 \nL 191.411519 217.143354 \nL 194.485899 218.504322 \nL 197.560279 219.252001 \nL 200.634659 218.84192 \nL 203.709039 220.683619 \nL 206.783419 220.460105 \nL 209.8578 220.785434 \nL 212.93218 219.458029 \nL 216.00656 220.939144 \nL 219.08094 221.324265 \nL 222.15532 221.242616 \nL 225.2297 221.248115 \nL 228.304081 221.395761 \nL 231.378461 221.601225 \nL 234.452841 221.933464 \nL 237.527221 222.33311 \nL 240.601601 221.765935 \nL 243.675981 222.287702 \nL 246.750362 223.972166 \nL 249.824742 223.004358 \nL 252.899122 222.802842 \nL 255.973502 223.472115 \nL 259.047882 223.973295 \nL 262.122262 223.56547 \nL 265.196643 224.315969 \nL 268.271023 224.682758 \nL 271.345403 224.786406 \nL 274.419783 225.12344 \nL 277.494163 224.90331 \nL 280.568543 224.810943 \nL 283.642924 225.077045 \nL 286.717304 225.118363 \nL 289.791684 225.030791 \nL 292.866064 225.499254 \nL 295.940444 225.569058 \nL 299.014824 225.811751 \nL 302.089205 226.334646 \nL 305.163585 226.483139 \nL 308.237965 225.905669 \nL 311.312345 227.45955 \nL 314.386725 226.95964 \nL 317.461105 227.685461 \nL 320.535486 225.957281 \nL 323.609866 227.174834 \nL 326.684246 227.838043 \nL 329.758626 227.115042 \nL 332.833006 227.373105 \nL 335.907386 227.605786 \nL 338.981767 228.291699 \nL 342.056147 225.315225 \nL 345.130527 226.625144 \nL 348.204907 229.141475 \nL 351.279287 229.254008 \nL 354.353667 228.45796 \nL 357.428048 228.706575 \nL 360.502428 228.932486 \nL 363.576808 228.957587 \nL 366.651188 229.874489 \nL 369.725568 229.700895 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p6dd6decf8e)\" d=\"M 65.361932 112.384079 \nL 68.436312 172.907314 \nL 71.510692 180.386221 \nL 74.585072 185.366565 \nL 77.659452 190.092935 \nL 80.733833 192.407469 \nL 83.808213 194.45703 \nL 86.882593 197.775332 \nL 89.956973 197.407274 \nL 93.031353 200.060957 \nL 96.105733 200.383325 \nL 99.180114 203.756061 \nL 102.254494 204.345658 \nL 105.328874 204.706806 \nL 108.403254 206.601528 \nL 111.477634 207.330029 \nL 114.552014 207.523365 \nL 117.626395 208.866988 \nL 120.700775 208.311094 \nL 123.775155 209.702804 \nL 126.849535 211.956982 \nL 129.923915 212.482839 \nL 132.998295 212.652202 \nL 136.072676 212.991351 \nL 139.147056 213.327962 \nL 142.221436 213.961557 \nL 145.295816 212.511043 \nL 148.370196 212.339423 \nL 151.444576 212.933392 \nL 154.518957 216.004631 \nL 157.593337 212.948199 \nL 160.667717 213.490133 \nL 163.742097 211.039516 \nL 166.816477 217.108945 \nL 169.890857 214.057168 \nL 172.965238 216.840023 \nL 176.039618 215.522912 \nL 179.113998 216.096293 \nL 182.188378 215.299257 \nL 185.262758 217.109791 \nL 188.337138 216.143675 \nL 191.411519 216.261566 \nL 194.485899 217.061422 \nL 197.560279 217.400007 \nL 200.634659 217.521001 \nL 203.709039 217.50013 \nL 206.783419 217.48208 \nL 209.8578 219.181492 \nL 212.93218 219.475797 \nL 216.00656 219.305306 \nL 219.08094 220.260846 \nL 222.15532 219.92903 \nL 225.2297 216.398072 \nL 228.304081 220.471527 \nL 231.378461 220.191747 \nL 234.452841 218.255565 \nL 237.527221 221.108507 \nL 240.601601 220.437119 \nL 243.675981 219.558293 \nL 246.750362 221.536639 \nL 249.824742 219.32251 \nL 252.899122 220.799113 \nL 255.973502 221.618147 \nL 259.047882 221.932477 \nL 262.122262 223.31206 \nL 265.196643 222.372878 \nL 268.271023 222.295176 \nL 271.345403 223.744563 \nL 274.419783 222.883082 \nL 277.494163 217.581075 \nL 280.568543 222.33311 \nL 283.642924 221.48136 \nL 286.717304 220.47745 \nL 289.791684 222.548868 \nL 292.866064 223.479871 \nL 295.940444 223.170195 \nL 299.014824 223.560393 \nL 302.089205 222.147953 \nL 305.163585 222.723167 \nL 308.237965 220.775281 \nL 311.312345 223.614121 \nL 314.386725 224.585455 \nL 317.461105 223.616236 \nL 320.535486 219.660108 \nL 323.609866 223.660375 \nL 326.684246 224.036471 \nL 329.758626 223.503562 \nL 332.833006 225.762112 \nL 335.907386 222.605981 \nL 338.981767 225.113287 \nL 342.056147 222.441553 \nL 345.130527 224.346288 \nL 348.204907 220.984975 \nL 351.279287 224.508318 \nL 354.353667 225.876901 \nL 357.428048 225.660156 \nL 360.502428 223.969346 \nL 363.576808 225.282227 \nL 366.651188 223.940155 \nL 369.725568 224.06961 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Model Loss -->\n    <g transform=\"translate(184.460313 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"385.994141\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"447.175781\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"499.275391\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 279.742188 105.099375 \nL 377.94375 105.099375 \nQ 379.94375 105.099375 379.94375 103.099375 \nL 379.94375 29.318125 \nQ 379.94375 27.318125 377.94375 27.318125 \nL 279.742188 27.318125 \nQ 277.742188 27.318125 277.742188 29.318125 \nL 277.742188 103.099375 \nQ 277.742188 105.099375 279.742188 105.099375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 281.742188 35.416562 \nL 301.742188 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_14\">\n     <!-- loss_baseline -->\n     <g transform=\"translate(309.742188 38.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"306.640625\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"367.919922\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"420.019531\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"481.542969\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"509.326172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"537.109375\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"600.488281\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 281.742188 50.372813 \nL 301.742188 50.372813 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_15\">\n     <!-- loss_A1 -->\n     <g transform=\"translate(309.742188 53.872813)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 281.742188 65.329062 \nL 301.742188 65.329062 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_16\">\n     <!-- loss_A2 -->\n     <g transform=\"translate(309.742188 68.829062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 281.742188 80.285313 \nL 301.742188 80.285313 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_17\">\n     <!-- loss_A3 -->\n     <g transform=\"translate(309.742188 83.785313)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 281.742188 95.241562 \nL 301.742188 95.241562 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_18\">\n     <!-- loss_A4 -->\n     <g transform=\"translate(309.742188 98.741562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-52\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6dd6decf8e\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 398.50625 277.314375\" width=\"398.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-15T23:20:09.861251</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 398.50625 277.314375 \nL 398.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \nL 391.30625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m03f5806499\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"71.724432\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(68.543182 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"133.212035\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(126.849535 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.699638\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(188.337138 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"256.187242\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(249.824742 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"317.674845\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(311.312345 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"379.162448\" xlink:href=\"#m03f5806499\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(369.618698 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epochs -->\n     <g transform=\"translate(206.073438 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0808c55f38\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"237.894724\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.710 -->\n      <g transform=\"translate(20.878125 241.693942)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"207.51877\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.715 -->\n      <g transform=\"translate(20.878125 211.317988)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"177.142816\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.720 -->\n      <g transform=\"translate(20.878125 180.942035)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"146.766862\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.725 -->\n      <g transform=\"translate(20.878125 150.566081)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"116.390908\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.730 -->\n      <g transform=\"translate(20.878125 120.190127)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"86.014954\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.735 -->\n      <g transform=\"translate(20.878125 89.814173)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"55.639\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.740 -->\n      <g transform=\"translate(20.878125 59.438219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m0808c55f38\" y=\"25.263046\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.745 -->\n      <g transform=\"translate(20.878125 29.062265)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- accuracy -->\n     <g transform=\"translate(14.798437 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pa78a01af54)\" d=\"M 71.724432 189.489855 \nL 74.798812 128.322303 \nL 77.873192 112.262743 \nL 80.947572 108.484129 \nL 84.021952 97.384022 \nL 87.096333 99.981796 \nL 90.170713 89.826795 \nL 93.245093 89.826795 \nL 96.319473 84.867342 \nL 99.393853 82.505663 \nL 102.468233 89.354242 \nL 105.542614 84.867342 \nL 108.616994 80.852271 \nL 111.691374 84.630884 \nL 114.765754 74.239788 \nL 117.840134 74.948074 \nL 120.914514 74.239788 \nL 123.988895 73.058586 \nL 127.063275 76.837562 \nL 130.137655 72.822491 \nL 133.212035 71.16946 \nL 136.286415 73.295044 \nL 139.360795 63.848329 \nL 142.435176 67.390847 \nL 145.509556 63.375776 \nL 148.583936 73.058586 \nL 151.658316 68.807782 \nL 154.732696 64.792711 \nL 157.807076 60.778002 \nL 160.881457 69.279973 \nL 163.955837 63.13968 \nL 167.030217 65.737455 \nL 170.104597 64.792711 \nL 173.178977 66.446103 \nL 176.253357 56.29074 \nL 179.327738 56.999388 \nL 182.402118 63.375776 \nL 185.476498 56.763293 \nL 188.550878 56.29074 \nL 191.625258 61.014097 \nL 194.699638 54.401614 \nL 197.774019 56.763293 \nL 200.848399 57.235484 \nL 203.922779 64.792711 \nL 206.997159 49.914352 \nL 210.071539 51.567382 \nL 213.145919 55.818549 \nL 216.2203 55.818549 \nL 219.29468 51.803478 \nL 222.36906 51.331287 \nL 225.44344 49.205703 \nL 228.51782 53.692965 \nL 231.5922 52.512126 \nL 234.666581 54.873805 \nL 237.740961 48.969608 \nL 240.815341 50.622638 \nL 243.889721 49.441799 \nL 246.964101 52.984317 \nL 250.038481 48.733512 \nL 253.112862 56.054644 \nL 256.187242 49.441799 \nL 259.261622 47.316578 \nL 262.336002 41.175923 \nL 265.410382 46.135738 \nL 268.484762 43.77406 \nL 271.559143 45.190994 \nL 274.633523 46.135738 \nL 277.707903 48.733512 \nL 280.782283 48.969608 \nL 283.856663 46.135738 \nL 286.931043 44.482346 \nL 290.005424 43.537602 \nL 293.079804 50.859096 \nL 296.154184 47.552673 \nL 299.228564 49.441799 \nL 302.302944 43.537602 \nL 305.377324 44.24625 \nL 308.451705 41.884572 \nL 311.526085 40.467637 \nL 314.600465 44.718441 \nL 317.674845 48.969608 \nL 320.749225 43.77406 \nL 323.823605 40.467637 \nL 326.897986 45.190994 \nL 329.972366 44.24625 \nL 333.046746 43.301507 \nL 336.121126 40.231541 \nL 339.195506 40.939828 \nL 342.269886 39.286798 \nL 345.344267 41.884572 \nL 348.418647 40.467637 \nL 351.493027 35.508184 \nL 354.567407 40.703732 \nL 357.641787 39.286798 \nL 360.716167 41.884572 \nL 363.790548 40.231541 \nL 366.864928 43.065411 \nL 369.939308 44.24625 \nL 373.013688 39.522893 \nL 376.088068 39.286798 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pa78a01af54)\" d=\"M 71.724432 203.659927 \nL 74.798812 116.9861 \nL 77.873192 112.026647 \nL 80.947572 110.13716 \nL 84.021952 92.424569 \nL 87.096333 98.09267 \nL 90.170713 95.258439 \nL 93.245093 85.103437 \nL 96.319473 87.701212 \nL 99.393853 86.284277 \nL 102.468233 88.173403 \nL 105.542614 97.620117 \nL 108.616994 81.560919 \nL 111.691374 81.797015 \nL 114.765754 78.254497 \nL 117.840134 74.239788 \nL 120.914514 76.365009 \nL 123.988895 74.475883 \nL 127.063275 78.018401 \nL 130.137655 71.16946 \nL 133.212035 72.114204 \nL 136.286415 73.058586 \nL 139.360795 63.375776 \nL 142.435176 70.460812 \nL 145.509556 76.601466 \nL 148.583936 63.13968 \nL 151.658316 71.405556 \nL 154.732696 62.903585 \nL 157.807076 69.043877 \nL 160.881457 69.279973 \nL 163.955837 65.737455 \nL 167.030217 61.722746 \nL 170.104597 65.265264 \nL 173.178977 62.194936 \nL 176.253357 71.405556 \nL 179.327738 62.431032 \nL 182.402118 53.929061 \nL 185.476498 57.944132 \nL 188.550878 58.652418 \nL 191.625258 66.682198 \nL 194.699638 56.763293 \nL 197.774019 53.929061 \nL 200.848399 60.778002 \nL 203.922779 61.014097 \nL 206.997159 52.039935 \nL 210.071539 56.526835 \nL 213.145919 56.29074 \nL 216.2203 53.929061 \nL 219.29468 49.205703 \nL 222.36906 51.803478 \nL 225.44344 57.471579 \nL 228.51782 49.678256 \nL 231.5922 60.305811 \nL 234.666581 50.859096 \nL 237.740961 62.194936 \nL 240.815341 45.190994 \nL 243.889721 46.371834 \nL 246.964101 52.512126 \nL 250.038481 52.748222 \nL 253.112862 46.371834 \nL 256.187242 48.260959 \nL 259.261622 43.537602 \nL 262.336002 50.622638 \nL 265.410382 54.165156 \nL 268.484762 50.859096 \nL 271.559143 50.150447 \nL 274.633523 47.08012 \nL 277.707903 54.873805 \nL 280.782283 55.1099 \nL 283.856663 48.260959 \nL 286.931043 49.205703 \nL 290.005424 42.59322 \nL 293.079804 51.331287 \nL 296.154184 44.718441 \nL 299.228564 44.718441 \nL 302.302944 44.482346 \nL 305.377324 45.190994 \nL 308.451705 46.135738 \nL 311.526085 47.08012 \nL 314.600465 40.467637 \nL 317.674845 48.733512 \nL 320.749225 52.984317 \nL 323.823605 37.161214 \nL 326.897986 44.482346 \nL 329.972366 47.316578 \nL 333.046746 46.135738 \nL 336.121126 49.441799 \nL 339.195506 46.607929 \nL 342.269886 44.718441 \nL 345.344267 50.386543 \nL 348.418647 38.342054 \nL 351.493027 38.814245 \nL 354.567407 39.522893 \nL 357.641787 41.648476 \nL 360.716167 43.065411 \nL 363.790548 40.939828 \nL 366.864928 49.914352 \nL 369.939308 41.648476 \nL 373.013688 47.788769 \nL 376.088068 36.452566 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pa78a01af54)\" d=\"M 71.724432 174.138581 \nL 74.798812 128.085846 \nL 77.873192 115.805261 \nL 80.947572 103.052123 \nL 84.021952 108.720225 \nL 87.096333 116.9861 \nL 90.170713 113.443582 \nL 93.245093 107.066832 \nL 96.319473 98.09267 \nL 99.393853 98.800957 \nL 102.468233 97.856575 \nL 105.542614 93.605408 \nL 108.616994 91.479825 \nL 111.691374 101.399093 \nL 114.765754 95.022343 \nL 117.840134 90.298986 \nL 120.914514 90.771539 \nL 123.988895 92.89676 \nL 127.063275 85.575628 \nL 130.137655 85.811724 \nL 133.212035 78.726688 \nL 136.286415 93.841504 \nL 139.360795 87.229021 \nL 142.435176 94.314057 \nL 145.509556 81.324824 \nL 148.583936 84.394789 \nL 151.658316 78.963145 \nL 154.732696 85.339533 \nL 157.807076 80.852271 \nL 160.881457 78.726688 \nL 163.955837 81.088366 \nL 167.030217 88.882051 \nL 170.104597 84.158694 \nL 173.178977 71.405556 \nL 176.253357 78.726688 \nL 179.327738 77.782306 \nL 182.402118 71.16946 \nL 185.476498 71.641651 \nL 188.550878 75.656722 \nL 191.625258 82.505663 \nL 194.699638 77.309753 \nL 197.774019 73.058586 \nL 200.848399 69.988621 \nL 203.922779 65.97355 \nL 206.997159 70.696907 \nL 210.071539 76.837562 \nL 213.145919 77.545848 \nL 216.2203 69.516068 \nL 219.29468 86.756468 \nL 222.36906 70.224717 \nL 225.44344 67.390847 \nL 228.51782 66.210008 \nL 231.5922 72.114204 \nL 234.666581 75.892818 \nL 237.740961 65.97355 \nL 240.815341 71.877747 \nL 243.889721 73.295044 \nL 246.964101 59.361067 \nL 250.038481 62.903585 \nL 253.112862 62.431032 \nL 256.187242 66.210008 \nL 259.261622 67.154389 \nL 262.336002 65.029168 \nL 265.410382 69.279973 \nL 268.484762 69.516068 \nL 271.559143 65.97355 \nL 274.633523 70.224717 \nL 277.707903 72.3503 \nL 280.782283 67.626942 \nL 283.856663 69.752526 \nL 286.931043 67.626942 \nL 290.005424 63.13968 \nL 293.079804 58.416323 \nL 296.154184 69.516068 \nL 299.228564 61.014097 \nL 302.302944 65.737455 \nL 305.377324 62.431032 \nL 308.451705 71.16946 \nL 311.526085 65.265264 \nL 314.600465 63.848329 \nL 317.674845 61.48665 \nL 320.749225 59.361067 \nL 323.823605 59.124971 \nL 326.897986 64.556615 \nL 329.972366 57.707674 \nL 333.046746 57.235484 \nL 336.121126 59.124971 \nL 339.195506 49.441799 \nL 342.269886 65.265264 \nL 345.344267 66.682198 \nL 348.418647 65.97355 \nL 351.493027 67.863038 \nL 354.567407 56.526835 \nL 357.641787 70.224717 \nL 360.716167 57.235484 \nL 363.790548 63.375776 \nL 366.864928 59.361067 \nL 369.939308 58.652418 \nL 373.013688 62.431032 \nL 376.088068 62.431032 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pa78a01af54)\" d=\"M 71.724432 189.017302 \nL 74.798812 113.679678 \nL 77.873192 105.413802 \nL 80.947572 100.92654 \nL 84.021952 93.605408 \nL 87.096333 95.494896 \nL 90.170713 94.550152 \nL 93.245093 79.435336 \nL 96.319473 83.21395 \nL 99.393853 82.741759 \nL 102.468233 76.837562 \nL 105.542614 69.516068 \nL 108.616994 73.767235 \nL 111.691374 72.3503 \nL 114.765754 73.058586 \nL 117.840134 70.696907 \nL 120.914514 73.531139 \nL 123.988895 68.099133 \nL 127.063275 57.471579 \nL 130.137655 65.97355 \nL 133.212035 67.626942 \nL 136.286415 63.611871 \nL 139.360795 61.014097 \nL 142.435176 61.958841 \nL 145.509556 65.737455 \nL 148.583936 67.154389 \nL 151.658316 55.345996 \nL 154.732696 60.541906 \nL 157.807076 59.124971 \nL 160.881457 56.999388 \nL 163.955837 54.637709 \nL 167.030217 54.873805 \nL 170.104597 53.45687 \nL 173.178977 52.748222 \nL 176.253357 51.095191 \nL 179.327738 58.888514 \nL 182.402118 55.1099 \nL 185.476498 45.899281 \nL 188.550878 47.08012 \nL 191.625258 45.42709 \nL 194.699638 49.678256 \nL 197.774019 55.582453 \nL 200.848399 45.190994 \nL 203.922779 47.788769 \nL 206.997159 44.010155 \nL 210.071539 52.512126 \nL 213.145919 46.844025 \nL 216.2203 44.010155 \nL 219.29468 51.567382 \nL 222.36906 46.135738 \nL 225.44344 46.607929 \nL 228.51782 44.24625 \nL 231.5922 46.844025 \nL 234.666581 51.567382 \nL 237.740961 48.733512 \nL 240.815341 47.08012 \nL 243.889721 46.844025 \nL 246.964101 44.482346 \nL 250.038481 45.190994 \nL 253.112862 48.260959 \nL 256.187242 46.844025 \nL 259.261622 45.663185 \nL 262.336002 43.065411 \nL 265.410382 44.954899 \nL 268.484762 40.703732 \nL 271.559143 41.412381 \nL 274.633523 40.703732 \nL 277.707903 45.42709 \nL 280.782283 42.59322 \nL 283.856663 44.718441 \nL 286.931043 43.537602 \nL 290.005424 39.522893 \nL 293.079804 44.482346 \nL 296.154184 44.954899 \nL 299.228564 35.508184 \nL 302.302944 42.356763 \nL 305.377324 38.814245 \nL 308.451705 33.382601 \nL 311.526085 40.703732 \nL 314.600465 43.77406 \nL 317.674845 37.161214 \nL 320.749225 38.105958 \nL 323.823605 38.578149 \nL 326.897986 42.356763 \nL 329.972366 38.105958 \nL 333.046746 39.995084 \nL 336.121126 35.271726 \nL 339.195506 41.648476 \nL 342.269886 44.24625 \nL 345.344267 38.814245 \nL 348.418647 41.412381 \nL 351.493027 41.412381 \nL 354.567407 39.286798 \nL 357.641787 37.39731 \nL 360.716167 42.120667 \nL 363.790548 39.522893 \nL 366.864928 37.869863 \nL 369.939308 38.814245 \nL 373.013688 32.201761 \nL 376.088068 37.39731 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pa78a01af54)\" d=\"M 71.724432 229.874489 \nL 74.798812 104.941611 \nL 77.873192 95.494896 \nL 80.947572 85.811724 \nL 84.021952 86.520372 \nL 87.096333 87.937307 \nL 90.170713 76.128913 \nL 93.245093 77.782306 \nL 96.319473 78.490592 \nL 99.393853 74.711979 \nL 102.468233 72.822491 \nL 105.542614 67.154389 \nL 108.616994 65.97355 \nL 111.691374 70.933365 \nL 114.765754 68.335229 \nL 117.840134 68.335229 \nL 120.914514 69.752526 \nL 123.988895 69.279973 \nL 127.063275 63.13968 \nL 130.137655 64.792711 \nL 133.212035 63.611871 \nL 136.286415 60.541906 \nL 139.360795 56.999388 \nL 142.435176 61.958841 \nL 145.509556 60.778002 \nL 148.583936 62.194936 \nL 151.658316 61.250193 \nL 154.732696 58.180227 \nL 157.807076 62.194936 \nL 160.881457 51.567382 \nL 163.955837 64.084424 \nL 167.030217 59.361067 \nL 170.104597 67.626942 \nL 173.178977 56.054644 \nL 176.253357 62.194936 \nL 179.327738 67.154389 \nL 182.402118 65.265264 \nL 185.476498 61.014097 \nL 188.550878 60.541906 \nL 191.625258 59.124971 \nL 194.699638 63.13968 \nL 197.774019 58.652418 \nL 200.848399 62.431032 \nL 203.922779 62.903585 \nL 206.997159 64.084424 \nL 210.071539 58.180227 \nL 213.145919 63.13968 \nL 216.2203 57.944132 \nL 219.29468 54.401614 \nL 222.36906 61.250193 \nL 225.44344 57.235484 \nL 228.51782 53.929061 \nL 231.5922 71.641651 \nL 234.666581 57.944132 \nL 237.740961 54.401614 \nL 240.815341 58.888514 \nL 243.889721 52.512126 \nL 246.964101 58.416323 \nL 250.038481 60.541906 \nL 253.112862 50.859096 \nL 256.187242 63.611871 \nL 259.261622 58.652418 \nL 262.336002 54.873805 \nL 265.410382 53.220775 \nL 268.484762 51.803478 \nL 271.559143 52.748222 \nL 274.633523 56.054644 \nL 277.707903 50.859096 \nL 280.782283 55.582453 \nL 283.856663 62.194936 \nL 286.931043 56.526835 \nL 290.005424 53.45687 \nL 293.079804 57.471579 \nL 296.154184 51.567382 \nL 299.228564 46.371834 \nL 302.302944 53.929061 \nL 305.377324 47.316578 \nL 308.451705 56.29074 \nL 311.526085 55.582453 \nL 314.600465 56.29074 \nL 317.674845 55.582453 \nL 320.749225 54.637709 \nL 323.823605 50.386543 \nL 326.897986 66.682198 \nL 329.972366 48.733512 \nL 333.046746 54.165156 \nL 336.121126 52.039935 \nL 339.195506 45.42709 \nL 342.269886 50.150447 \nL 345.344267 44.482346 \nL 348.418647 50.622638 \nL 351.493027 59.833258 \nL 354.567407 53.929061 \nL 357.641787 52.512126 \nL 360.716167 49.914352 \nL 363.790548 44.482346 \nL 366.864928 47.552673 \nL 369.939308 48.969608 \nL 373.013688 49.678256 \nL 376.088068 52.039935 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 391.30625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Model Accuracy -->\n    <g transform=\"translate(176.589687 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-77\"/>\n     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"398.689453\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"453.669922\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"508.650391\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"572.029297\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"613.142578\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"674.421875\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"729.402344\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 260.301563 234.758125 \nL 384.30625 234.758125 \nQ 386.30625 234.758125 386.30625 232.758125 \nL 386.30625 158.976875 \nQ 386.30625 156.976875 384.30625 156.976875 \nL 260.301563 156.976875 \nQ 258.301563 156.976875 258.301563 158.976875 \nL 258.301563 232.758125 \nQ 258.301563 234.758125 260.301563 234.758125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 262.301563 165.075312 \nL 282.301563 165.075312 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_18\">\n     <!-- accuracy_baseline -->\n     <g transform=\"translate(290.301563 168.575312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"564.648438\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"625.927734\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"678.027344\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"739.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"767.333984\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"795.117188\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"858.496094\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 262.301563 180.031562 \nL 282.301563 180.031562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_19\">\n     <!-- accuracy_A1 -->\n     <g transform=\"translate(290.301563 183.531562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 262.301563 194.987812 \nL 282.301563 194.987812 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_20\">\n     <!-- accuracy_A2 -->\n     <g transform=\"translate(290.301563 198.487812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_26\">\n     <path d=\"M 262.301563 209.944062 \nL 282.301563 209.944062 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_27\"/>\n    <g id=\"text_21\">\n     <!-- accuracy_A3 -->\n     <g transform=\"translate(290.301563 213.444062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 262.301563 224.900312 \nL 282.301563 224.900312 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_29\"/>\n    <g id=\"text_22\">\n     <!-- accuracy_A4 -->\n     <g transform=\"translate(290.301563 228.400312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-52\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa78a01af54\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "print(\"Original Model Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_baseline, model_accuracy_baseline = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_baseline}, Accuracy: {model_accuracy_baseline}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Model Results\n",
      "268/268 - 0s - loss: 0.5611 - accuracy: 0.7307\n",
      "Loss: 0.561127245426178, Accuracy: 0.7307288646697998\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "print(\"Alternative Model 1 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A1, model_accuracy_A1 = nn_A1.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A1}, Accuracy: {model_accuracy_A1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 1 Results\n",
      "268/268 - 0s - loss: 0.5581 - accuracy: 0.7294\n",
      "Loss: 0.5580662488937378, Accuracy: 0.7294460535049438\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "print(\"Alternative Model 2 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A2, model_accuracy_A2 = nn_A2.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A2}, Accuracy: {model_accuracy_A2}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 2 Results\n",
      "268/268 - 0s - loss: 0.5651 - accuracy: 0.7255\n",
      "Loss: 0.5651211738586426, Accuracy: 0.7254810333251953\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "print(\"Alternative Model 3 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A3, model_accuracy_A3 = nn_A3.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A3}, Accuracy: {model_accuracy_A3}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 3 Results\n",
      "268/268 - 0s - loss: 0.5734 - accuracy: 0.7314\n",
      "Loss: 0.5733740925788879, Accuracy: 0.7314285635948181\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "print(\"Alternative Model 4 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A4, model_accuracy_A4 = nn_A4.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A4}, Accuracy: {model_accuracy_A4}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 4 Results\n",
      "268/268 - 0s - loss: 0.5711 - accuracy: 0.7305\n",
      "Loss: 0.5711314678192139, Accuracy: 0.7304956316947937\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Save each of your alternative models as an HDF5 file.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A1.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A1.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A2.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A2.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A3.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A3.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A4.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A4.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('dev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "930eee74a1f2acacbb765ecf4f41bca31b45440fd68d4dff19855f34b00f2967"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}