{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Venture Funding with Deep Learning\n",
    "\n",
    "You work as a risk management associate at Alphabet Soup, a venture capital firm. Alphabet Soup’s business team receives many funding applications from startups every day. This team has asked you to help them create a model that predicts whether applicants will be successful if funded by Alphabet Soup.\n",
    "\n",
    "The business team has given you a CSV containing more than 34,000 organizations that have received funding from Alphabet Soup over the years. With your knowledge of machine learning and neural networks, you decide to use the features in the provided dataset to create a binary classifier model that will predict whether an applicant will become a successful business. The CSV file contains a variety of information about these businesses, including whether or not they ultimately became successful.\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "The steps for this challenge are broken out into the following sections:\n",
    "\n",
    "* Prepare the data for use on a neural network model.\n",
    "\n",
    "* Compile and evaluate a binary classification model using a neural network.\n",
    "\n",
    "* Optimize the neural network model.\n",
    "\n",
    "### Prepare the Data for Use on a Neural Network Model \n",
    "\n",
    "Using your knowledge of Pandas and scikit-learn’s `StandardScaler()`, preprocess the dataset so that you can use it to compile and evaluate the neural network model later.\n",
    "\n",
    "Open the starter code file, and complete the following data preparation steps:\n",
    "\n",
    "1. Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.   \n",
    "\n",
    "2. Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model.\n",
    " \n",
    "3. Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame.\n",
    "\n",
    "4. Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "> **Note** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. \n",
    "\n",
    "5. Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. \n",
    "\n",
    "6. Split the features and target sets into training and testing datasets.\n",
    "\n",
    "7. Use scikit-learn's `StandardScaler` to scale the features data.\n",
    "\n",
    "### Compile and Evaluate a Binary Classification Model Using a Neural Network\n",
    "\n",
    "Use your knowledge of TensorFlow to design a binary classification deep neural network model. This model should use the dataset’s features to predict whether an Alphabet Soup&ndash;funded startup will be successful based on the features in the dataset. Consider the number of inputs before determining the number of layers that your model will contain or the number of neurons on each layer. Then, compile and fit your model. Finally, evaluate your binary classification model to calculate the model’s loss and accuracy. \n",
    " \n",
    "To do so, complete the following steps:\n",
    "\n",
    "1. Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n",
    "\n",
    "2. Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n",
    "\n",
    "> **Hint** When fitting the model, start with a small number of epochs, such as 20, 50, or 100.\n",
    "\n",
    "3. Evaluate the model using the test data to determine the model’s loss and accuracy.\n",
    "\n",
    "4. Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. \n",
    "\n",
    "### Optimize the Neural Network Model\n",
    "\n",
    "Using your knowledge of TensorFlow and Keras, optimize your model to improve the model's accuracy. Even if you do not successfully achieve a better accuracy, you'll need to demonstrate at least two attempts to optimize the model. You can include these attempts in your existing notebook. Or, you can make copies of the starter notebook in the same folder, rename them, and code each model optimization in a new notebook. \n",
    "\n",
    "> **Note** You will not lose points if your model does not achieve a high accuracy, as long as you make at least two attempts to optimize the model.\n",
    "\n",
    "To do so, complete the following steps:\n",
    "\n",
    "1. Define at least three new deep neural network models (the original plus 2 optimization attempts). With each, try to improve on your first model’s predictive accuracy.\n",
    "\n",
    "> **Rewind** Recall that perfect accuracy has a value of 1, so accuracy improves as its value moves closer to 1. To optimize your model for a predictive accuracy as close to 1 as possible, you can use any or all of the following techniques:\n",
    ">\n",
    "> * Adjust the input data by dropping different features columns to ensure that no variables or outliers confuse the model.\n",
    ">\n",
    "> * Add more neurons (nodes) to a hidden layer.\n",
    ">\n",
    "> * Add more hidden layers.\n",
    ">\n",
    "> * Use different activation functions for the hidden layers.\n",
    ">\n",
    "> * Add to or reduce the number of epochs in the training regimen.\n",
    "\n",
    "2. After finishing your models, display the accuracy scores achieved by each model, and compare the results.\n",
    "\n",
    "3. Save each of your models as an HDF5 file.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Prepare the data to be used on a neural network model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Read the `applicants_data.csv` file into a Pandas DataFrame. Review the DataFrame, looking for categorical variables that will need to be encoded, as well as columns that could eventually define your features and target variables.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read the applicants_data.csv file from the Resources folder into a Pandas DataFrame\n",
    "applicant_data_df = pd.read_csv(\n",
    "    Path('./Resources/applicants_data.csv')\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Review the data types associated with the columns\n",
    "applicant_data_df.dtypes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EIN                        int64\n",
       "NAME                      object\n",
       "APPLICATION_TYPE          object\n",
       "AFFILIATION               object\n",
       "CLASSIFICATION            object\n",
       "USE_CASE                  object\n",
       "ORGANIZATION              object\n",
       "STATUS                     int64\n",
       "INCOME_AMT                object\n",
       "SPECIAL_CONSIDERATIONS    object\n",
       "ASK_AMT                    int64\n",
       "IS_SUCCESSFUL              int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Drop the “EIN” (Employer Identification Number) and “NAME” columns from the DataFrame, because they are not relevant to the binary classification model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
    "applicant_data_df = applicant_data_df.drop(columns=['EIN', 'NAME'])\n",
    "\n",
    "# Review the DataFrame\n",
    "applicant_data_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0              T10       Independent          C1000    ProductDev   \n",
       "1               T3       Independent          C2000  Preservation   \n",
       "2               T5  CompanySponsored          C3000    ProductDev   \n",
       "3               T3  CompanySponsored          C2000  Preservation   \n",
       "4               T3       Independent          C1000     Heathcare   \n",
       "\n",
       "   ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  \\\n",
       "0   Association       1              0                      N     5000   \n",
       "1  Co-operative       1         1-9999                      N   108590   \n",
       "2   Association       1              0                      N     5000   \n",
       "3         Trust       1    10000-24999                      N     6692   \n",
       "4         Trust       1  100000-499999                      N   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Encode the dataset’s categorical variables using `OneHotEncoder`, and then place the encoded variables into a new DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables = list(applicant_data_df.dtypes[applicant_data_df.dtypes == \"object\"].index)\n",
    "\n",
    "# Display the categorical variables list\n",
    "categorical_variables"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT',\n",
       " 'SPECIAL_CONSIDERATIONS']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Encode the categorcal variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_data,\n",
    "    columns = enc.get_feature_names(categorical_variables)\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_1-9999  INCOME_AMT_10000-24999  \\\n",
       "0                   0.0  ...                0.0                     0.0   \n",
       "1                   0.0  ...                1.0                     0.0   \n",
       "2                   0.0  ...                0.0                     0.0   \n",
       "3                   0.0  ...                0.0                     1.0   \n",
       "4                   0.0  ...                0.0                     0.0   \n",
       "\n",
       "   INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                       0.0                 0.0               0.0   \n",
       "1                       0.0                 0.0               0.0   \n",
       "2                       0.0                 0.0               0.0   \n",
       "3                       0.0                 0.0               0.0   \n",
       "4                       1.0                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                       1.0                       0.0  \n",
       "1                       1.0                       0.0  \n",
       "2                       1.0                       0.0  \n",
       "3                       1.0                       0.0  \n",
       "4                       1.0                       0.0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Add the original DataFrame’s numerical variables to the DataFrame containing the encoded variables.\n",
    "\n",
    "> **Note** To complete this step, you will employ the Pandas `concat()` function that was introduced earlier in this course. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "encoded_df = pd.concat([encoded_df, applicant_data_df.drop(columns=categorical_variables)], axis=1)\n",
    "\n",
    "# Review the Dataframe\n",
    "encoded_df.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  \\\n",
       "0                   0.0  ...                 0.0               0.0   \n",
       "1                   0.0  ...                 0.0               0.0   \n",
       "2                   0.0  ...                 0.0               0.0   \n",
       "3                   0.0  ...                 0.0               0.0   \n",
       "4                   0.0  ...                 0.0               0.0   \n",
       "\n",
       "   INCOME_AMT_25000-99999  INCOME_AMT_50M+  INCOME_AMT_5M-10M  \\\n",
       "0                     0.0              0.0                0.0   \n",
       "1                     0.0              0.0                0.0   \n",
       "2                     0.0              0.0                0.0   \n",
       "3                     0.0              0.0                0.0   \n",
       "4                     0.0              0.0                0.0   \n",
       "\n",
       "   SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  STATUS  ASK_AMT  \\\n",
       "0                       1.0                       0.0       1     5000   \n",
       "1                       1.0                       0.0       1   108590   \n",
       "2                       1.0                       0.0       1     5000   \n",
       "3                       1.0                       0.0       1     6692   \n",
       "4                       1.0                       0.0       1   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  \n",
       "\n",
       "[5 rows x 117 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Using the preprocessed data, create the features (`X`) and target (`y`) datasets. The target dataset should be defined by the preprocessed DataFrame column “IS_SUCCESSFUL”. The remaining columns should define the features dataset. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Define the target set y using the IS_SUCCESSFUL column\n",
    "y = encoded_df['IS_SUCCESSFUL']\n",
    "\n",
    "# Display a sample of y\n",
    "y.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: IS_SUCCESSFUL, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
    "X = encoded_df.drop(columns=['IS_SUCCESSFUL'])\n",
    "\n",
    "# Review the features DataFrame\n",
    "X.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   APPLICATION_TYPE_T10  APPLICATION_TYPE_T12  APPLICATION_TYPE_T13  \\\n",
       "0                   1.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T14  APPLICATION_TYPE_T15  APPLICATION_TYPE_T17  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T19  APPLICATION_TYPE_T2  APPLICATION_TYPE_T25  \\\n",
       "0                   0.0                  0.0                   0.0   \n",
       "1                   0.0                  0.0                   0.0   \n",
       "2                   0.0                  0.0                   0.0   \n",
       "3                   0.0                  0.0                   0.0   \n",
       "4                   0.0                  0.0                   0.0   \n",
       "\n",
       "   APPLICATION_TYPE_T29  ...  INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  \\\n",
       "0                   0.0  ...                       0.0                 0.0   \n",
       "1                   0.0  ...                       0.0                 0.0   \n",
       "2                   0.0  ...                       0.0                 0.0   \n",
       "3                   0.0  ...                       0.0                 0.0   \n",
       "4                   0.0  ...                       1.0                 0.0   \n",
       "\n",
       "   INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  INCOME_AMT_50M+  \\\n",
       "0               0.0                     0.0              0.0   \n",
       "1               0.0                     0.0              0.0   \n",
       "2               0.0                     0.0              0.0   \n",
       "3               0.0                     0.0              0.0   \n",
       "4               0.0                     0.0              0.0   \n",
       "\n",
       "   INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \\\n",
       "0                0.0                       1.0                       0.0   \n",
       "1                0.0                       1.0                       0.0   \n",
       "2                0.0                       1.0                       0.0   \n",
       "3                0.0                       1.0                       0.0   \n",
       "4                0.0                       1.0                       0.0   \n",
       "\n",
       "   STATUS  ASK_AMT  \n",
       "0       1     5000  \n",
       "1       1   108590  \n",
       "2       1     5000  \n",
       "3       1     6692  \n",
       "4       1   142590  \n",
       "\n",
       "[5 rows x 116 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T12</th>\n",
       "      <th>APPLICATION_TYPE_T13</th>\n",
       "      <th>APPLICATION_TYPE_T14</th>\n",
       "      <th>APPLICATION_TYPE_T15</th>\n",
       "      <th>APPLICATION_TYPE_T17</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T2</th>\n",
       "      <th>APPLICATION_TYPE_T25</th>\n",
       "      <th>APPLICATION_TYPE_T29</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>108590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>142590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6: Split the features and target sets into training and testing datasets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "# Assign the function a random_state equal to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 7: Use scikit-learn's `StandardScaler` to scale the features data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Compile and Evaluate a Binary Classification Model Using a Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Create a deep neural network by assigning the number of input features, the number of layers, and the number of neurons on each layer using Tensorflow’s Keras.\n",
    "\n",
    "> **Hint** You can start with a two-layer deep neural network model that uses the `relu` activation function for both layers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "source": [
    "# Define global number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Define activation function\n",
    "act_function_hidden = 'relu'\n",
    "act_function_out = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1 =  (number_input_features + number_output_neurons) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2 =  (hidden_nodes_layer1 + number_output_neurons) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=act_function_hidden))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn.add(Dense(units=hidden_nodes_layer2, activation=act_function_hidden))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn.add(Dense(units=number_output_neurons, activation=act_function_out))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_113 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Compile and fit the model using the `binary_crossentropy` loss function, the `adam` optimizer, and the `accuracy` evaluation metric.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "# Compile the Sequential model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_baseline = nn.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 986us/step - loss: 0.5781 - accuracy: 0.7180\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 951us/step - loss: 0.5549 - accuracy: 0.7280\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5505 - accuracy: 0.7307\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5489 - accuracy: 0.7313\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5466 - accuracy: 0.7331\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 927us/step - loss: 0.5452 - accuracy: 0.7327\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 943us/step - loss: 0.5443 - accuracy: 0.7344\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 954us/step - loss: 0.5435 - accuracy: 0.7344\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5432 - accuracy: 0.7352\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5421 - accuracy: 0.7356\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5418 - accuracy: 0.7345\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5413 - accuracy: 0.7352\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7358\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7352\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5395 - accuracy: 0.7369\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 963us/step - loss: 0.5392 - accuracy: 0.7368\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 947us/step - loss: 0.5383 - accuracy: 0.7369\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 969us/step - loss: 0.5385 - accuracy: 0.7371\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7365\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5376 - accuracy: 0.7372\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 943us/step - loss: 0.5374 - accuracy: 0.7374\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 982us/step - loss: 0.5368 - accuracy: 0.7371\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5366 - accuracy: 0.7386\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5361 - accuracy: 0.7381\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5363 - accuracy: 0.7387\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 955us/step - loss: 0.5361 - accuracy: 0.7371\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 950us/step - loss: 0.5359 - accuracy: 0.7378\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5355 - accuracy: 0.7392\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5354 - accuracy: 0.7378\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5351 - accuracy: 0.7388\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 987us/step - loss: 0.5349 - accuracy: 0.7383\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 981us/step - loss: 0.5347 - accuracy: 0.7385\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5342 - accuracy: 0.7382\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5348 - accuracy: 0.7399\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5341 - accuracy: 0.7398\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 990us/step - loss: 0.5339 - accuracy: 0.7387\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5337 - accuracy: 0.7398\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5337 - accuracy: 0.7399\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5331 - accuracy: 0.7391\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 903us/step - loss: 0.5334 - accuracy: 0.7402\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5329 - accuracy: 0.7398\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 997us/step - loss: 0.5332 - accuracy: 0.7397\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 959us/step - loss: 0.5328 - accuracy: 0.7385\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 900us/step - loss: 0.5331 - accuracy: 0.7409\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 917us/step - loss: 0.5325 - accuracy: 0.7407\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5325 - accuracy: 0.7400\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 908us/step - loss: 0.5323 - accuracy: 0.7406\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 937us/step - loss: 0.5323 - accuracy: 0.7407\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 936us/step - loss: 0.5315 - accuracy: 0.7411\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5324 - accuracy: 0.7403\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 928us/step - loss: 0.5320 - accuracy: 0.7405\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 916us/step - loss: 0.5320 - accuracy: 0.7401\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5317 - accuracy: 0.7411\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 907us/step - loss: 0.5317 - accuracy: 0.7408\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7410\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5318 - accuracy: 0.7404\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5315 - accuracy: 0.7411\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 935us/step - loss: 0.5318 - accuracy: 0.7399\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 912us/step - loss: 0.5309 - accuracy: 0.7410\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 963us/step - loss: 0.5311 - accuracy: 0.7414\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5310 - accuracy: 0.7424\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5309 - accuracy: 0.7416\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5313 - accuracy: 0.7420\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 922us/step - loss: 0.5308 - accuracy: 0.7417\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5309 - accuracy: 0.7416\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 942us/step - loss: 0.5305 - accuracy: 0.7411\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5307 - accuracy: 0.7411\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7416\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7418\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7420\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5304 - accuracy: 0.7408\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7413\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 950us/step - loss: 0.5304 - accuracy: 0.7410\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7420\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5297 - accuracy: 0.7419\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7423\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7425\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5300 - accuracy: 0.7418\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 951us/step - loss: 0.5299 - accuracy: 0.7411\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 926us/step - loss: 0.5297 - accuracy: 0.7420\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 934us/step - loss: 0.5297 - accuracy: 0.7425\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5294 - accuracy: 0.7417\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 932us/step - loss: 0.5292 - accuracy: 0.7419\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 933us/step - loss: 0.5294 - accuracy: 0.7420\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 915us/step - loss: 0.5293 - accuracy: 0.7425\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5292 - accuracy: 0.7424\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 910us/step - loss: 0.5295 - accuracy: 0.7427\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 907us/step - loss: 0.5296 - accuracy: 0.7423\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 914us/step - loss: 0.5300 - accuracy: 0.7425\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7433\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5292 - accuracy: 0.7425\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 933us/step - loss: 0.5290 - accuracy: 0.7427\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 924us/step - loss: 0.5293 - accuracy: 0.7423\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5294 - accuracy: 0.7425\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5287 - accuracy: 0.7421\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5289 - accuracy: 0.7419\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 921us/step - loss: 0.5286 - accuracy: 0.7427\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5285 - accuracy: 0.7427\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Evaluate the model using the test data to determine the model’s loss and accuracy.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_baseline, model_accuracy_baseline = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_baseline}, Accuracy: {model_accuracy_baseline}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "268/268 - 0s - loss: 0.5663 - accuracy: 0.7313\n",
      "Loss: 0.5663371682167053, Accuracy: 0.7313119769096375\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Save and export your model to an HDF5 file, and name the file `AlphabetSoup.h5`. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "# Set the model's file path\n",
    "file_path = Path(\"./Resources/AlphabetSoup.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Optimize the neural network model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Define at least three new deep neural network models (resulting in the original plus 3 optimization attempts). With each, try to improve on your first model’s predictive accuracy.\n",
    "\n",
    "> **Rewind** Recall that perfect accuracy has a value of 1, so accuracy improves as its value moves closer to 1. To optimize your model for a predictive accuracy as close to 1 as possible, you can use any or all of the following techniques:\n",
    ">\n",
    "> * Adjust the input data by dropping different features columns to ensure that no variables or outliers confuse the model.\n",
    ">\n",
    "> * Add more neurons (nodes) to a hidden layer.\n",
    ">\n",
    "> * Add more hidden layers.\n",
    ">\n",
    "> * Use different activation functions for the hidden layers.\n",
    ">\n",
    "> * Add to or reduce the number of epochs in the training regimen.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Alternative Model 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A1 = 'relu'\n",
    "act_function_out_A1 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A1 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A1}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A1 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A1 =  (number_input_features_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A1 =  (hidden_nodes_layer1_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer2_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer3_A1 = (hidden_nodes_layer2_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 3: {hidden_nodes_layer3_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer4_A1 = (hidden_nodes_layer3_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 4: {hidden_nodes_layer4_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer5_A1 = (hidden_nodes_layer4_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 5: {hidden_nodes_layer5_A1}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer6_A1 = (hidden_nodes_layer5_A1 + number_output_neurons_A1) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 6: {hidden_nodes_layer6_A1}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A1 = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer1_A1, input_dim=number_input_features_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer2_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer3_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer4_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer5_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A1.add(Dense(units=hidden_nodes_layer6_A1, activation=act_function_hidden_A1))\n",
    "\n",
    "# Output layer\n",
    "nn_A1.add(Dense(units=number_output_neurons_A1, activation=act_function_out_A1))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A1.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 29\n",
      "Hidden layer 3: 15\n",
      "Hidden layer 4: 8\n",
      "Hidden layer 5: 4\n",
      "Hidden layer 6: 2\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9,124\n",
      "Trainable params: 9,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A1 = nn_A1.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.6109 - accuracy: 0.7156\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5795 - accuracy: 0.7299\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 982us/step - loss: 0.5676 - accuracy: 0.7307\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 932us/step - loss: 0.5607 - accuracy: 0.7310\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 942us/step - loss: 0.5567 - accuracy: 0.7339\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 983us/step - loss: 0.5552 - accuracy: 0.7330\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5533 - accuracy: 0.7335\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5513 - accuracy: 0.7352\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5494 - accuracy: 0.7347\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5482 - accuracy: 0.7350\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5480 - accuracy: 0.7346\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 981us/step - loss: 0.5476 - accuracy: 0.7331\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 946us/step - loss: 0.5465 - accuracy: 0.7357\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7357\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5452 - accuracy: 0.7363\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 967us/step - loss: 0.5447 - accuracy: 0.7369\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 993us/step - loss: 0.5446 - accuracy: 0.7366\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5436 - accuracy: 0.7369\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 964us/step - loss: 0.5436 - accuracy: 0.7363\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 925us/step - loss: 0.5423 - accuracy: 0.7374\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 955us/step - loss: 0.5429 - accuracy: 0.7373\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5423 - accuracy: 0.7371\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5415 - accuracy: 0.7387\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 972us/step - loss: 0.5414 - accuracy: 0.7376\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7365\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 958us/step - loss: 0.5406 - accuracy: 0.7388\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5413 - accuracy: 0.7374\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 999us/step - loss: 0.5405 - accuracy: 0.7388\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 961us/step - loss: 0.5401 - accuracy: 0.7378\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 956us/step - loss: 0.5399 - accuracy: 0.7378\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5396 - accuracy: 0.7383\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5394 - accuracy: 0.7390\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 957us/step - loss: 0.5396 - accuracy: 0.7384\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 941us/step - loss: 0.5387 - accuracy: 0.7389\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5390 - accuracy: 0.7374\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5387 - accuracy: 0.7389\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5383 - accuracy: 0.7403\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5376 - accuracy: 0.7396\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5383 - accuracy: 0.7395\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 974us/step - loss: 0.5381 - accuracy: 0.7382\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5373 - accuracy: 0.7398\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 984us/step - loss: 0.5373 - accuracy: 0.7403\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 965us/step - loss: 0.5377 - accuracy: 0.7392\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7391\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5371 - accuracy: 0.7406\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 960us/step - loss: 0.5374 - accuracy: 0.7399\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5367 - accuracy: 0.7399\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7403\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 997us/step - loss: 0.5367 - accuracy: 0.7411\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5367 - accuracy: 0.7406\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7397\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5364 - accuracy: 0.7410\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7392\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 987us/step - loss: 0.5361 - accuracy: 0.7408\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 949us/step - loss: 0.5358 - accuracy: 0.7389\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5350 - accuracy: 0.7417\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5343 - accuracy: 0.7415\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5352 - accuracy: 0.7405\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 991us/step - loss: 0.5351 - accuracy: 0.7405\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5343 - accuracy: 0.7415\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 977us/step - loss: 0.5340 - accuracy: 0.7412\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5337 - accuracy: 0.7420\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7408\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 968us/step - loss: 0.5332 - accuracy: 0.7402\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7408\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5331 - accuracy: 0.7409\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5339 - accuracy: 0.7414\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7401\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7401\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 988us/step - loss: 0.5331 - accuracy: 0.7412\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5330 - accuracy: 0.7411\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7421\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 983us/step - loss: 0.5326 - accuracy: 0.7407\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5326 - accuracy: 0.7418\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7418\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7418\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5326 - accuracy: 0.7417\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5320 - accuracy: 0.7416\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5327 - accuracy: 0.7414\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5317 - accuracy: 0.7425\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7411\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7404\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5324 - accuracy: 0.7430\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 971us/step - loss: 0.5321 - accuracy: 0.7418\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7414\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7416\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7410\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7415\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7418\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7409\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7428\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5308 - accuracy: 0.7428\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 996us/step - loss: 0.5308 - accuracy: 0.7427\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 994us/step - loss: 0.5308 - accuracy: 0.7423\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7421\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7424\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7409\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7423\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7413\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7432\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A2 = 'selu'\n",
    "act_function_out_A2 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A2 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A2}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A2 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A2 =  (number_input_features_A2 + number_output_neurons_A2) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A2}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A2 =  (hidden_nodes_layer1_A2 + number_output_neurons_A2) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1_A2}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A2.add(Dense(units=hidden_nodes_layer1_A2, input_dim=number_input_features_A2, activation=act_function_hidden_A2))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A2.add(Dense(units=hidden_nodes_layer2_A2, activation=act_function_hidden_A2))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A2.add(Dense(units=number_output_neurons_A2, activation=act_function_out_A2))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A2.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A2 = nn_A2.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 890us/step - loss: 0.5887 - accuracy: 0.7205\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5610 - accuracy: 0.7281\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5560 - accuracy: 0.7301\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5519 - accuracy: 0.7322\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5520 - accuracy: 0.7313\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7299\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5492 - accuracy: 0.7305\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 944us/step - loss: 0.5481 - accuracy: 0.7315\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5470 - accuracy: 0.7330\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 934us/step - loss: 0.5468 - accuracy: 0.7329\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5463 - accuracy: 0.7331\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 788us/step - loss: 0.5458 - accuracy: 0.7338\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7341\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 763us/step - loss: 0.5449 - accuracy: 0.7325\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 938us/step - loss: 0.5445 - accuracy: 0.7335\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 711us/step - loss: 0.5434 - accuracy: 0.7343\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 716us/step - loss: 0.5437 - accuracy: 0.7342\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 719us/step - loss: 0.5437 - accuracy: 0.7339\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.7351\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5430 - accuracy: 0.7350\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7362\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5424 - accuracy: 0.7337\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5421 - accuracy: 0.7348\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7336\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5412 - accuracy: 0.7358\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5412 - accuracy: 0.7353\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 994us/step - loss: 0.5413 - accuracy: 0.7362\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5413 - accuracy: 0.7351\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 845us/step - loss: 0.5409 - accuracy: 0.7358\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 809us/step - loss: 0.5404 - accuracy: 0.7362\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 826us/step - loss: 0.5401 - accuracy: 0.7358\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 862us/step - loss: 0.5409 - accuracy: 0.7345\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 858us/step - loss: 0.5400 - accuracy: 0.7353\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 839us/step - loss: 0.5398 - accuracy: 0.7374\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 836us/step - loss: 0.5399 - accuracy: 0.7362\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5395 - accuracy: 0.7364\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 848us/step - loss: 0.5397 - accuracy: 0.7374\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 840us/step - loss: 0.5392 - accuracy: 0.7374\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 852us/step - loss: 0.5395 - accuracy: 0.7367\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 848us/step - loss: 0.5388 - accuracy: 0.7356\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 853us/step - loss: 0.5394 - accuracy: 0.7364\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 843us/step - loss: 0.5387 - accuracy: 0.7371\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 827us/step - loss: 0.5386 - accuracy: 0.7376\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 826us/step - loss: 0.5393 - accuracy: 0.7383\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 881us/step - loss: 0.5381 - accuracy: 0.7375\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5385 - accuracy: 0.7365\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 834us/step - loss: 0.5388 - accuracy: 0.7364\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 861us/step - loss: 0.5380 - accuracy: 0.7377\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 842us/step - loss: 0.5382 - accuracy: 0.7349\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5379 - accuracy: 0.7376\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 842us/step - loss: 0.5380 - accuracy: 0.7381\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 873us/step - loss: 0.5379 - accuracy: 0.7383\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 843us/step - loss: 0.5373 - accuracy: 0.7373\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 970us/step - loss: 0.5370 - accuracy: 0.7367\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 844us/step - loss: 0.5371 - accuracy: 0.7383\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 872us/step - loss: 0.5376 - accuracy: 0.7373\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1000us/step - loss: 0.5371 - accuracy: 0.7371\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 863us/step - loss: 0.5367 - accuracy: 0.7394\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 846us/step - loss: 0.5368 - accuracy: 0.7388\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5368 - accuracy: 0.7389\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5368 - accuracy: 0.7383\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5362 - accuracy: 0.7381\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 859us/step - loss: 0.5361 - accuracy: 0.7385\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5363 - accuracy: 0.7378\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 865us/step - loss: 0.5363 - accuracy: 0.7377\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 978us/step - loss: 0.5366 - accuracy: 0.7383\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 892us/step - loss: 0.5363 - accuracy: 0.7376\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 896us/step - loss: 0.5369 - accuracy: 0.7372\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 913us/step - loss: 0.5360 - accuracy: 0.7380\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 883us/step - loss: 0.5363 - accuracy: 0.7377\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 838us/step - loss: 0.5361 - accuracy: 0.7380\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 873us/step - loss: 0.5362 - accuracy: 0.7388\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 886us/step - loss: 0.5360 - accuracy: 0.7395\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 939us/step - loss: 0.5362 - accuracy: 0.7377\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 887us/step - loss: 0.5362 - accuracy: 0.7391\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5359 - accuracy: 0.7383\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5353 - accuracy: 0.7389\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 893us/step - loss: 0.5358 - accuracy: 0.7374\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 944us/step - loss: 0.5356 - accuracy: 0.7384\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5356 - accuracy: 0.7386\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 971us/step - loss: 0.5355 - accuracy: 0.7390\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 882us/step - loss: 0.5350 - accuracy: 0.7394\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5355 - accuracy: 0.7394\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 850us/step - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 881us/step - loss: 0.5349 - accuracy: 0.7397\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 851us/step - loss: 0.5349 - accuracy: 0.7397\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 930us/step - loss: 0.5350 - accuracy: 0.7394\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 900us/step - loss: 0.5348 - accuracy: 0.7410\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 922us/step - loss: 0.5353 - accuracy: 0.7384\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 957us/step - loss: 0.5347 - accuracy: 0.7382\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 864us/step - loss: 0.5352 - accuracy: 0.7383\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5348 - accuracy: 0.7380\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 866us/step - loss: 0.5351 - accuracy: 0.7399\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 874us/step - loss: 0.5349 - accuracy: 0.7376\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 857us/step - loss: 0.5346 - accuracy: 0.7397\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7387\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7394\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 962us/step - loss: 0.5347 - accuracy: 0.7395\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5349 - accuracy: 0.7389\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7389\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A3 = 'gelu'\n",
    "act_function_out_A3 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A3 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A3}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A3 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A3 =  (number_input_features_A3 + number_output_neurons_A3) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A3}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A3 =  (hidden_nodes_layer1_A3 + number_output_neurons_A3) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer1_A3}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A3 = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "nn_A3.add(Dense(units=hidden_nodes_layer1_A3, input_dim=number_input_features_A3, activation=act_function_hidden_A3))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A3.add(Dense(units=hidden_nodes_layer2_A3, activation=act_function_hidden_A3))\n",
    "\n",
    "# Add the output layer to the model specifying the number of output neurons and activation function\n",
    "nn_A3.add(Dense(units=number_output_neurons_A3, activation=act_function_out_A3))\n",
    "\n",
    "# Display the Sequential model summary\n",
    "nn_A3.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 58\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_107 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 30        \n",
      "=================================================================\n",
      "Total params: 8,527\n",
      "Trainable params: 8,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A3 = nn_A3.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 1s 952us/step - loss: 0.5767 - accuracy: 0.7180\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5531 - accuracy: 0.7304\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7318\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 947us/step - loss: 0.5482 - accuracy: 0.7325\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 958us/step - loss: 0.5460 - accuracy: 0.7338\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5451 - accuracy: 0.7334\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5448 - accuracy: 0.7336\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5429 - accuracy: 0.7361\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 995us/step - loss: 0.5429 - accuracy: 0.7355\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7355\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5406 - accuracy: 0.7365\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5403 - accuracy: 0.7377\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 992us/step - loss: 0.5406 - accuracy: 0.7370\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 980us/step - loss: 0.5401 - accuracy: 0.7372\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 985us/step - loss: 0.5394 - accuracy: 0.7371\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5391 - accuracy: 0.7375\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 989us/step - loss: 0.5380 - accuracy: 0.7371\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5378 - accuracy: 0.7379\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7397\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7383\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7380\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7387\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5360 - accuracy: 0.7391\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5358 - accuracy: 0.7390\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5356 - accuracy: 0.7383\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5352 - accuracy: 0.7381\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5351 - accuracy: 0.7400\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7392\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7394\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7398\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5342 - accuracy: 0.7402\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5337 - accuracy: 0.7401\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7404\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7405\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5334 - accuracy: 0.7407\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5333 - accuracy: 0.7395\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7401\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7416\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7414\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7417\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5325 - accuracy: 0.7410\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7400\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5322 - accuracy: 0.7417\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7413\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5321 - accuracy: 0.7419\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5313 - accuracy: 0.7405\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7414\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7419\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7407\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7416\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7415\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7419\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7414\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5310 - accuracy: 0.7407\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7411\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7414\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7414\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5308 - accuracy: 0.7418\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7417\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7412\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7414\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5304 - accuracy: 0.7416\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7421\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7418\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7425\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7423\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5296 - accuracy: 0.7425\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7417\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7421\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7418\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7420\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7427\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7433\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7422\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7428\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5289 - accuracy: 0.7437\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5288 - accuracy: 0.7425\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7420\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7430\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7429\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7428\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5290 - accuracy: 0.7422\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5285 - accuracy: 0.7429\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7426\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5286 - accuracy: 0.7434\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7423\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5284 - accuracy: 0.7419\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5281 - accuracy: 0.7428\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.7423\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5288 - accuracy: 0.7423\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5277 - accuracy: 0.7427\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5277 - accuracy: 0.7430\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5280 - accuracy: 0.7422\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5279 - accuracy: 0.7427\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5278 - accuracy: 0.7429\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5278 - accuracy: 0.7428\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5274 - accuracy: 0.7439\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5275 - accuracy: 0.7430\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Model 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "# Define activation function\n",
    "act_function_hidden_A4 = 'gelu'\n",
    "act_function_out_A4 = 'sigmoid'\n",
    "\n",
    "# Define the the number of inputs (features) to the model\n",
    "number_input_features_A4 = len(X_train.iloc[0])\n",
    "\n",
    "# Review the number of features\n",
    "print(f'Number of features: {number_input_features_A4}')\n",
    "\n",
    "# Define the number of neurons in the output layer\n",
    "number_output_neurons_A4 = 1\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer1_A4 =  (number_input_features_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the first layer\n",
    "print(f'Hidden layer 1: {hidden_nodes_layer1_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the second hidden layer\n",
    "hidden_nodes_layer2_A4 =  (hidden_nodes_layer1_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 2: {hidden_nodes_layer2_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer3_A4 = (hidden_nodes_layer2_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number hidden nodes in the second layer\n",
    "print(f'Hidden layer 3: {hidden_nodes_layer3_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer4_A4 = (hidden_nodes_layer3_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 4: {hidden_nodes_layer4_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer5_A4 = (hidden_nodes_layer4_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 5: {hidden_nodes_layer5_A4}')\n",
    "\n",
    "# Define the number of hidden nodes for the first hidden layer\n",
    "hidden_nodes_layer6_A4 = (hidden_nodes_layer5_A4 + number_output_neurons_A4) // 2\n",
    "\n",
    "# Review the number of hidden nodes in the first layer\n",
    "print(f'Hidden layer 6: {hidden_nodes_layer6_A4}')\n",
    "\n",
    "# Create the Sequential model instance\n",
    "nn_A4 = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer1_A4, input_dim=number_input_features_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer2_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer3_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer4_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer5_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Add the second hidden layer\n",
    "nn_A4.add(Dense(units=hidden_nodes_layer6_A4, activation=act_function_hidden_A4))\n",
    "\n",
    "# Output layer\n",
    "nn_A4.add(Dense(units=number_output_neurons_A4, activation=act_function_out_A4))\n",
    "\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_A4.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features: 116\n",
      "Hidden layer 1: 58\n",
      "Hidden layer 2: 29\n",
      "Hidden layer 3: 15\n",
      "Hidden layer 4: 8\n",
      "Hidden layer 5: 4\n",
      "Hidden layer 6: 2\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 58)                6786      \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 29)                1711      \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 15)                450       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 8)                 128       \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9,124\n",
      "Trainable params: 9,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "source": [
    "# Compile the Sequential model\n",
    "nn_A4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using 50 epochs and the training data\n",
    "fit_model_A4 = nn_A4.fit(X_train_scaled, y_train, epochs=epochs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 2s 1ms/step - loss: 0.5771 - accuracy: 0.7113\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5515 - accuracy: 0.7319\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5483 - accuracy: 0.7334\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7350\n",
      "Epoch 5/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5442 - accuracy: 0.7349\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.7347\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5424 - accuracy: 0.7366\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7364\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5411 - accuracy: 0.7362\n",
      "Epoch 10/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5400 - accuracy: 0.7369\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7372\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7381\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5382 - accuracy: 0.7383\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.7375\n",
      "Epoch 15/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7379\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7379\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5368 - accuracy: 0.7377\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7378\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7388\n",
      "Epoch 20/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5359 - accuracy: 0.7385\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5350 - accuracy: 0.7387\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7392\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7398\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7390\n",
      "Epoch 25/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5344 - accuracy: 0.7392\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5347 - accuracy: 0.7391\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5348 - accuracy: 0.7396\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7389\n",
      "Epoch 30/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5333 - accuracy: 0.7407\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5345 - accuracy: 0.7386\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5343 - accuracy: 0.7394\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5354 - accuracy: 0.7380\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7399\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5329 - accuracy: 0.7381\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5335 - accuracy: 0.7384\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7391\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5336 - accuracy: 0.7392\n",
      "Epoch 40/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7394\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5332 - accuracy: 0.7388\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7395\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5328 - accuracy: 0.7389\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5327 - accuracy: 0.7388\n",
      "Epoch 45/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7386\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7396\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7388\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7396\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7402\n",
      "Epoch 50/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7391\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5315 - accuracy: 0.7397\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5316 - accuracy: 0.7403\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5331 - accuracy: 0.7374\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7396\n",
      "Epoch 55/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5315 - accuracy: 0.7402\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5323 - accuracy: 0.7395\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7405\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5314 - accuracy: 0.7395\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5318 - accuracy: 0.7392\n",
      "Epoch 60/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5309 - accuracy: 0.7408\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5319 - accuracy: 0.7387\n",
      "Epoch 62/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7395\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7401\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7404\n",
      "Epoch 65/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7406\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7405\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7399\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7408\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5303 - accuracy: 0.7400\n",
      "Epoch 70/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5326 - accuracy: 0.7389\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7399\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5309 - accuracy: 0.7404\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5314 - accuracy: 0.7397\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7407\n",
      "Epoch 75/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7415\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7403\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7414\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5307 - accuracy: 0.7399\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5304 - accuracy: 0.7400\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7399\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7400\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5296 - accuracy: 0.7402\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7409\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5317 - accuracy: 0.7382\n",
      "Epoch 85/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5300 - accuracy: 0.7411\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7402\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5301 - accuracy: 0.7406\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7417\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7409\n",
      "Epoch 90/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7418\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5305 - accuracy: 0.7408\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7393\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5312 - accuracy: 0.7403\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5297 - accuracy: 0.7405\n",
      "Epoch 95/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7409\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5292 - accuracy: 0.7418\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7413\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5293 - accuracy: 0.7411\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7410\n",
      "Epoch 100/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5298 - accuracy: 0.7406\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: After finishing your models, display the accuracy scores achieved by each model, and compare the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "# Create a DataFrame using the model history for loss\n",
    "models_plot_loss = pd.DataFrame(\n",
    "    {'loss_baseline' : fit_model_baseline.history['loss'],\n",
    "    'loss_A1' : fit_model_A1.history['loss'],\n",
    "    'loss_A2' : fit_model_A2.history['loss'],\n",
    "    'loss_A3' : fit_model_A3.history['loss'],\n",
    "    'loss_A4' : fit_model_A4.history['loss']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Vizualize the model plot where the y-axis displays the loss metric\n",
    "models_plot_loss.plot()\n",
    "\n",
    "# Create a DataFrame using the model history for accuracy\n",
    "models_plot_accuracy = pd.DataFrame(\n",
    "    {'accuracy_baseline' : fit_model_baseline.history['accuracy'],\n",
    "    'accuracy_A1' : fit_model_A1.history['accuracy'],\n",
    "    'accuracy_A2' : fit_model_A2.history['accuracy'],\n",
    "    'accuracy_A3' : fit_model_A3.history['accuracy'],\n",
    "    'accuracy_A4' : fit_model_A4.history['accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Vizualize the model plot where the y-axis displays the accuracy metric\n",
    "models_plot_accuracy.plot()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "execution_count": 178
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-15T23:06:36.515730</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 378.465625 248.518125 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \nL 371.265625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m43a6d60741\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.17141\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(106.80891 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.659013\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(168.296513 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"236.146617\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(229.784117 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.63422\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(291.27172 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.121823\" xlink:href=\"#m43a6d60741\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(349.578073 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mcbb2a1707f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mcbb2a1707f\" y=\"184.928781\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.54 -->\n      <g transform=\"translate(7.2 188.728)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mcbb2a1707f\" y=\"137.610881\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.56 -->\n      <g transform=\"translate(7.2 141.4101)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mcbb2a1707f\" y=\"90.292981\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.58 -->\n      <g transform=\"translate(7.2 94.0922)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mcbb2a1707f\" y=\"42.975081\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.60 -->\n      <g transform=\"translate(7.2 46.7743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#pfbe2d05af1)\" d=\"M 51.683807 94.681934 \nL 54.758187 149.791616 \nL 57.832567 160.173809 \nL 60.906947 163.907974 \nL 63.981327 169.322655 \nL 67.055708 172.620791 \nL 70.130088 174.775975 \nL 73.204468 176.645878 \nL 76.278848 177.344624 \nL 79.353228 180.005216 \nL 82.427608 180.701001 \nL 85.501989 181.741998 \nL 88.576369 184.515829 \nL 91.650749 184.989368 \nL 94.725129 185.997649 \nL 97.799509 186.906936 \nL 100.873889 189.012339 \nL 103.94827 188.386077 \nL 107.02265 188.697445 \nL 110.09703 190.50389 \nL 113.17141 191.099128 \nL 116.24579 192.541746 \nL 119.32017 192.937584 \nL 122.394551 194.267246 \nL 125.468931 193.657624 \nL 128.543311 194.178264 \nL 131.617691 194.550975 \nL 134.692071 195.469851 \nL 137.766451 195.688429 \nL 140.840832 195.8561 \nL 143.915212 196.568806 \nL 146.989592 197.042205 \nL 150.063972 197.372611 \nL 153.138352 198.630071 \nL 156.212732 197.179698 \nL 159.287113 198.818331 \nL 162.361493 199.365341 \nL 165.435873 199.765269 \nL 168.510253 199.860738 \nL 171.584633 201.304484 \nL 174.659013 200.441593 \nL 177.733394 201.704976 \nL 180.807774 201.027101 \nL 183.882154 201.889992 \nL 186.956534 201.257666 \nL 190.030914 202.612288 \nL 193.105294 201.87025 \nL 196.179675 202.575905 \nL 199.254055 203.080328 \nL 202.328435 203.173682 \nL 205.402815 205.128619 \nL 208.477195 203.010101 \nL 211.551575 203.856775 \nL 214.625956 203.838442 \nL 217.700336 204.469358 \nL 220.774716 204.553828 \nL 223.849096 204.112582 \nL 226.923476 204.416336 \nL 229.997856 205.040342 \nL 233.072237 204.299995 \nL 236.146617 206.396374 \nL 239.220997 205.918463 \nL 242.295377 206.154669 \nL 245.369757 206.395951 \nL 248.444137 205.531226 \nL 251.518518 206.776136 \nL 254.592898 206.444884 \nL 257.667278 207.372926 \nL 260.741658 206.834518 \nL 263.816038 207.230215 \nL 266.890418 207.012906 \nL 269.964799 206.966229 \nL 273.039179 207.74874 \nL 276.113559 208.159385 \nL 279.187939 207.54525 \nL 282.262319 208.362734 \nL 285.336699 209.404154 \nL 288.41108 208.207049 \nL 291.48546 209.92155 \nL 294.55984 208.570454 \nL 297.63422 208.864054 \nL 300.7086 209.279212 \nL 303.78298 209.215894 \nL 306.857361 209.9451 \nL 309.931741 210.371117 \nL 313.006121 210.033378 \nL 316.080501 210.140834 \nL 319.154881 210.433024 \nL 322.229261 209.817479 \nL 325.303642 209.636129 \nL 328.378022 208.674666 \nL 331.452402 211.948125 \nL 334.526782 210.524968 \nL 337.601162 210.962265 \nL 340.675542 210.240957 \nL 343.749923 209.999815 \nL 346.824303 211.742661 \nL 349.898683 211.134308 \nL 352.973063 211.827413 \nL 356.047443 212.1801 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#pfbe2d05af1)\" d=\"M 51.683807 17.083636 \nL 54.758187 91.382669 \nL 57.832567 119.705637 \nL 60.906947 135.848428 \nL 63.981327 145.482801 \nL 67.055708 149.031527 \nL 70.130088 153.395621 \nL 73.204468 158.077712 \nL 76.278848 162.797314 \nL 79.353228 165.625013 \nL 82.427608 165.889281 \nL 85.501989 166.85977 \nL 88.576369 169.561822 \nL 91.650749 170.767952 \nL 94.725129 172.589626 \nL 97.799509 173.707197 \nL 100.873889 174.108958 \nL 103.94827 176.34297 \nL 107.02265 176.443798 \nL 110.09703 179.533087 \nL 113.17141 178.009807 \nL 116.24579 179.490358 \nL 119.32017 181.315136 \nL 122.394551 181.644413 \nL 125.468931 182.688513 \nL 128.543311 183.539418 \nL 131.617691 181.841557 \nL 134.692071 183.741215 \nL 137.766451 184.614401 \nL 140.840832 185.089068 \nL 143.915212 185.812915 \nL 146.989592 186.441998 \nL 150.063972 185.783866 \nL 153.138352 188.032967 \nL 156.212732 187.226624 \nL 159.287113 187.975854 \nL 162.361493 189.019954 \nL 165.435873 190.608385 \nL 168.510253 188.841002 \nL 171.584633 189.333438 \nL 174.659013 191.371576 \nL 177.733394 191.241134 \nL 180.807774 190.297721 \nL 183.882154 191.771927 \nL 186.956534 191.757543 \nL 190.030914 191.183034 \nL 193.105294 192.705186 \nL 196.179675 193.143471 \nL 199.254055 192.742556 \nL 202.328435 192.635241 \nL 205.402815 192.157612 \nL 208.477195 193.455263 \nL 211.551575 193.772695 \nL 214.625956 194.08364 \nL 217.700336 194.889983 \nL 220.774716 196.642418 \nL 223.849096 198.467618 \nL 226.923476 196.236849 \nL 229.997856 196.465158 \nL 233.072237 198.365239 \nL 236.146617 199.097406 \nL 239.220997 199.866379 \nL 242.295377 197.946132 \nL 245.369757 200.932478 \nL 248.444137 200.561599 \nL 251.518518 201.356943 \nL 254.592898 199.292152 \nL 257.667278 201.069265 \nL 260.741658 201.149082 \nL 263.816038 201.337623 \nL 266.890418 201.372737 \nL 269.964799 202.421772 \nL 273.039179 202.342097 \nL 276.113559 202.516537 \nL 279.187939 203.42963 \nL 282.262319 202.441374 \nL 285.336699 202.385813 \nL 288.41108 203.955347 \nL 291.48546 202.267357 \nL 294.55984 204.560456 \nL 297.63422 203.27451 \nL 300.7086 204.691039 \nL 303.78298 202.918862 \nL 306.857361 203.605339 \nL 309.931741 205.34339 \nL 313.006121 204.68977 \nL 316.080501 206.01774 \nL 319.154881 205.51374 \nL 322.229261 205.467486 \nL 325.303642 206.097133 \nL 328.378022 205.905489 \nL 331.452402 206.613119 \nL 334.526782 206.591261 \nL 337.601162 206.704217 \nL 340.675542 208.110029 \nL 343.749923 205.186578 \nL 346.824303 206.003497 \nL 349.898683 206.874144 \nL 352.973063 207.950537 \nL 356.047443 208.677345 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pfbe2d05af1)\" d=\"M 51.683807 69.824774 \nL 54.758187 135.130504 \nL 57.832567 147.06277 \nL 60.906947 156.720129 \nL 63.981327 156.630723 \nL 67.055708 160.220063 \nL 70.130088 163.092324 \nL 73.204468 165.766596 \nL 76.278848 168.438611 \nL 79.353228 168.811604 \nL 82.427608 170.092756 \nL 85.501989 171.250939 \nL 88.576369 173.631188 \nL 91.650749 173.252131 \nL 94.725129 174.316537 \nL 97.799509 176.892378 \nL 100.873889 176.20026 \nL 103.94827 176.243552 \nL 107.02265 177.339406 \nL 110.09703 177.908556 \nL 113.17141 180.378351 \nL 116.24579 179.146979 \nL 119.32017 179.951911 \nL 122.394551 180.534176 \nL 125.468931 182.041662 \nL 128.543311 181.97764 \nL 131.617691 181.78374 \nL 134.692071 181.804892 \nL 137.766451 182.849556 \nL 140.840832 183.994202 \nL 143.915212 184.618913 \nL 146.989592 182.759586 \nL 150.063972 184.875284 \nL 153.138352 185.44655 \nL 156.212732 185.072569 \nL 159.287113 186.045455 \nL 162.361493 185.71519 \nL 165.435873 186.735316 \nL 168.510253 186.027404 \nL 171.584633 187.845271 \nL 174.659013 186.398987 \nL 177.733394 188.037903 \nL 180.807774 188.350117 \nL 183.882154 186.544659 \nL 186.956534 189.524236 \nL 190.030914 188.394397 \nL 193.105294 187.718073 \nL 196.179675 189.759595 \nL 199.254055 189.07199 \nL 202.328435 189.794568 \nL 205.402815 189.565977 \nL 208.477195 189.808811 \nL 211.551575 191.249313 \nL 214.625956 192.02167 \nL 217.700336 191.690418 \nL 220.774716 190.566784 \nL 223.849096 191.681957 \nL 226.923476 192.818847 \nL 229.997856 192.511145 \nL 233.072237 192.465737 \nL 236.146617 192.422021 \nL 239.220997 193.89383 \nL 242.295377 194.165431 \nL 245.369757 193.731941 \nL 248.444137 193.655368 \nL 251.518518 193.040105 \nL 254.592898 193.656214 \nL 257.667278 192.267042 \nL 260.741658 194.348896 \nL 263.816038 193.738851 \nL 266.890418 194.120446 \nL 269.964799 194.03372 \nL 273.039179 194.500773 \nL 276.113559 193.911316 \nL 279.187939 193.890022 \nL 282.262319 194.705954 \nL 285.336699 196.007977 \nL 288.41108 194.973184 \nL 291.48546 195.432481 \nL 294.55984 195.33264 \nL 297.63422 195.49467 \nL 300.7086 196.857048 \nL 303.78298 195.517938 \nL 306.857361 195.512015 \nL 309.931741 196.897379 \nL 313.006121 196.890328 \nL 316.080501 196.642982 \nL 319.154881 197.143738 \nL 322.229261 196.112894 \nL 325.303642 197.429018 \nL 328.378022 196.370676 \nL 331.452402 197.274321 \nL 334.526782 196.580511 \nL 337.601162 196.912327 \nL 340.675542 197.614034 \nL 343.749923 197.252463 \nL 346.824303 197.345112 \nL 349.898683 197.53591 \nL 352.973063 196.881444 \nL 356.047443 197.573562 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pfbe2d05af1)\" d=\"M 51.683807 98.068348 \nL 54.758187 153.901031 \nL 57.832567 160.308481 \nL 60.906947 165.562542 \nL 63.981327 170.751594 \nL 67.055708 172.792552 \nL 70.130088 173.529937 \nL 73.204468 178.049151 \nL 76.278848 177.991052 \nL 79.353228 180.533753 \nL 82.427608 183.494151 \nL 85.501989 184.305288 \nL 88.576369 183.5868 \nL 91.650749 184.739766 \nL 94.725129 186.414923 \nL 97.799509 187.114091 \nL 100.873889 189.658485 \nL 103.94827 190.177856 \nL 107.02265 191.760363 \nL 110.09703 191.630486 \nL 113.17141 192.710968 \nL 116.24579 192.847051 \nL 119.32017 194.393458 \nL 122.394551 194.94357 \nL 125.468931 195.313884 \nL 128.543311 196.236567 \nL 131.617691 196.405507 \nL 134.692071 197.2825 \nL 137.766451 197.712606 \nL 140.840832 197.625175 \nL 143.915212 198.600599 \nL 146.989592 199.929273 \nL 150.063972 201.062074 \nL 153.138352 200.468104 \nL 156.212732 200.555113 \nL 159.287113 200.779896 \nL 162.361493 202.349289 \nL 165.435873 200.932619 \nL 168.510253 202.022267 \nL 171.584633 203.30497 \nL 174.659013 202.684348 \nL 177.733394 202.025229 \nL 180.807774 203.386197 \nL 183.882154 204.133876 \nL 186.956534 203.723795 \nL 190.030914 205.565494 \nL 193.105294 205.34198 \nL 196.179675 205.667309 \nL 199.254055 204.339904 \nL 202.328435 205.821019 \nL 205.402815 206.20614 \nL 208.477195 206.124491 \nL 211.551575 206.12999 \nL 214.625956 206.277636 \nL 217.700336 206.4831 \nL 220.774716 206.815339 \nL 223.849096 207.214985 \nL 226.923476 206.64781 \nL 229.997856 207.169577 \nL 233.072237 208.854041 \nL 236.146617 207.886233 \nL 239.220997 207.684717 \nL 242.295377 208.35399 \nL 245.369757 208.85517 \nL 248.444137 208.447345 \nL 251.518518 209.197844 \nL 254.592898 209.564633 \nL 257.667278 209.668281 \nL 260.741658 210.005315 \nL 263.816038 209.785185 \nL 266.890418 209.692818 \nL 269.964799 209.95892 \nL 273.039179 210.000238 \nL 276.113559 209.912666 \nL 279.187939 210.381129 \nL 282.262319 210.450933 \nL 285.336699 210.693626 \nL 288.41108 211.216521 \nL 291.48546 211.365014 \nL 294.55984 210.787544 \nL 297.63422 212.341425 \nL 300.7086 211.841515 \nL 303.78298 212.567336 \nL 306.857361 210.839156 \nL 309.931741 212.056709 \nL 313.006121 212.719918 \nL 316.080501 211.996917 \nL 319.154881 212.25498 \nL 322.229261 212.487661 \nL 325.303642 213.173574 \nL 328.378022 210.1971 \nL 331.452402 211.507019 \nL 334.526782 214.02335 \nL 337.601162 214.135883 \nL 340.675542 213.339835 \nL 343.749923 213.58845 \nL 346.824303 213.814361 \nL 349.898683 213.839462 \nL 352.973063 214.756364 \nL 356.047443 214.58277 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pfbe2d05af1)\" d=\"M 51.683807 97.265954 \nL 54.758187 157.789189 \nL 57.832567 165.268096 \nL 60.906947 170.24844 \nL 63.981327 174.97481 \nL 67.055708 177.289344 \nL 70.130088 179.338905 \nL 73.204468 182.657207 \nL 76.278848 182.289149 \nL 79.353228 184.942832 \nL 82.427608 185.2652 \nL 85.501989 188.637936 \nL 88.576369 189.227533 \nL 91.650749 189.588681 \nL 94.725129 191.483403 \nL 97.799509 192.211904 \nL 100.873889 192.40524 \nL 103.94827 193.748863 \nL 107.02265 193.192969 \nL 110.09703 194.584679 \nL 113.17141 196.838857 \nL 116.24579 197.364714 \nL 119.32017 197.534077 \nL 122.394551 197.873226 \nL 125.468931 198.209837 \nL 128.543311 198.843432 \nL 131.617691 197.392918 \nL 134.692071 197.221298 \nL 137.766451 197.815267 \nL 140.840832 200.886506 \nL 143.915212 197.830074 \nL 146.989592 198.372008 \nL 150.063972 195.921391 \nL 153.138352 201.99082 \nL 156.212732 198.939043 \nL 159.287113 201.721898 \nL 162.361493 200.404787 \nL 165.435873 200.978168 \nL 168.510253 200.181132 \nL 171.584633 201.991666 \nL 174.659013 201.02555 \nL 177.733394 201.143441 \nL 180.807774 201.943297 \nL 183.882154 202.281882 \nL 186.956534 202.402876 \nL 190.030914 202.382005 \nL 193.105294 202.363955 \nL 196.179675 204.063367 \nL 199.254055 204.357672 \nL 202.328435 204.187181 \nL 205.402815 205.142721 \nL 208.477195 204.810905 \nL 211.551575 201.279947 \nL 214.625956 205.353402 \nL 217.700336 205.073622 \nL 220.774716 203.13744 \nL 223.849096 205.990382 \nL 226.923476 205.318994 \nL 229.997856 204.440168 \nL 233.072237 206.418514 \nL 236.146617 204.204385 \nL 239.220997 205.680988 \nL 242.295377 206.500022 \nL 245.369757 206.814352 \nL 248.444137 208.193935 \nL 251.518518 207.254753 \nL 254.592898 207.177051 \nL 257.667278 208.626438 \nL 260.741658 207.764957 \nL 263.816038 202.46295 \nL 266.890418 207.214985 \nL 269.964799 206.363235 \nL 273.039179 205.359325 \nL 276.113559 207.430743 \nL 279.187939 208.361746 \nL 282.262319 208.05207 \nL 285.336699 208.442268 \nL 288.41108 207.029828 \nL 291.48546 207.605042 \nL 294.55984 205.657156 \nL 297.63422 208.495996 \nL 300.7086 209.46733 \nL 303.78298 208.498111 \nL 306.857361 204.541983 \nL 309.931741 208.54225 \nL 313.006121 208.918346 \nL 316.080501 208.385437 \nL 319.154881 210.643987 \nL 322.229261 207.487856 \nL 325.303642 209.995162 \nL 328.378022 207.323428 \nL 331.452402 209.228163 \nL 334.526782 205.86685 \nL 337.601162 209.390193 \nL 340.675542 210.758776 \nL 343.749923 210.542031 \nL 346.824303 208.851221 \nL 349.898683 210.164102 \nL 352.973063 208.82203 \nL 356.047443 208.951485 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 224.64 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 371.265625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 371.265625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 266.064062 89.98125 \nL 364.265625 89.98125 \nQ 366.265625 89.98125 366.265625 87.98125 \nL 366.265625 14.2 \nQ 366.265625 12.2 364.265625 12.2 \nL 266.064062 12.2 \nQ 264.064062 12.2 264.064062 14.2 \nL 264.064062 87.98125 \nQ 264.064062 89.98125 266.064062 89.98125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 268.064062 20.298438 \nL 288.064062 20.298438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_11\">\n     <!-- loss_baseline -->\n     <g transform=\"translate(296.064062 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"306.640625\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"367.919922\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"420.019531\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"481.542969\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"509.326172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"537.109375\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"600.488281\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 268.064062 35.254688 \nL 288.064062 35.254688 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_12\">\n     <!-- loss_A1 -->\n     <g transform=\"translate(296.064062 38.754688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 268.064062 50.210938 \nL 288.064062 50.210938 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_13\">\n     <!-- loss_A2 -->\n     <g transform=\"translate(296.064062 53.710938)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 268.064062 65.167188 \nL 288.064062 65.167188 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_14\">\n     <!-- loss_A3 -->\n     <g transform=\"translate(296.064062 68.667188)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 268.064062 80.123437 \nL 288.064062 80.123437 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_15\">\n     <!-- loss_A4 -->\n     <g transform=\"translate(296.064062 83.623437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"193.164062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"243.164062\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"311.572266\" xlink:href=\"#DejaVuSans-52\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pfbe2d05af1\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"249.372422pt\" version=\"1.1\" viewBox=\"0 0 384.828125 249.372422\" width=\"384.828125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-15T23:06:36.820056</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 249.372422 \nL 384.828125 249.372422 \nL 384.828125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 42.828125 225.494297 \nL 377.628125 225.494297 \nL 377.628125 8.054297 \nL 42.828125 8.054297 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf3e5a907a5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.046307\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(54.865057 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.53391\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(113.17141 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"181.021513\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(174.659013 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.509117\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(236.146617 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"303.99672\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(297.63422 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.484323\" xlink:href=\"#mf3e5a907a5\" y=\"225.494297\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(355.940573 240.092735)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3f020d7fec\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"223.630896\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.710 -->\n      <g transform=\"translate(7.2 227.430115)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"193.254942\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.715 -->\n      <g transform=\"translate(7.2 197.054161)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"162.878988\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.720 -->\n      <g transform=\"translate(7.2 166.678207)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"132.503034\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.725 -->\n      <g transform=\"translate(7.2 136.302253)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"102.12708\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.730 -->\n      <g transform=\"translate(7.2 105.926299)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"71.751127\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.735 -->\n      <g transform=\"translate(7.2 75.550345)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"41.375173\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.740 -->\n      <g transform=\"translate(7.2 45.174391)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m3f020d7fec\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.745 -->\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pddd199b90c)\" d=\"M 58.046307 175.226027 \nL 61.120687 114.058475 \nL 64.195067 97.998915 \nL 67.269447 94.220302 \nL 70.343827 83.120194 \nL 73.418208 85.717969 \nL 76.492588 75.562967 \nL 79.566968 75.562967 \nL 82.641348 70.603514 \nL 85.715728 68.241836 \nL 88.790108 75.090414 \nL 91.864489 70.603514 \nL 94.938869 66.588443 \nL 98.013249 70.367057 \nL 101.087629 59.97596 \nL 104.162009 60.684246 \nL 107.236389 59.97596 \nL 110.31077 58.794759 \nL 113.38515 62.573734 \nL 116.45953 58.558663 \nL 119.53391 56.905633 \nL 122.60829 59.031216 \nL 125.68267 49.584501 \nL 128.757051 53.127019 \nL 131.831431 49.111948 \nL 134.905811 58.794759 \nL 137.980191 54.543954 \nL 141.054571 50.528883 \nL 144.128951 46.514174 \nL 147.203332 55.016145 \nL 150.277712 48.875853 \nL 153.352092 51.473627 \nL 156.426472 50.528883 \nL 159.500852 52.182275 \nL 162.575232 42.026912 \nL 165.649613 42.73556 \nL 168.723993 49.111948 \nL 171.798373 42.499465 \nL 174.872753 42.026912 \nL 177.947133 46.750269 \nL 181.021513 40.137786 \nL 184.095894 42.499465 \nL 187.170274 42.971656 \nL 190.244654 50.528883 \nL 193.319034 35.650524 \nL 196.393414 37.303554 \nL 199.467794 41.554721 \nL 202.542175 41.554721 \nL 205.616555 37.53965 \nL 208.690935 37.067459 \nL 211.765315 34.941876 \nL 214.839695 39.429138 \nL 217.914075 38.248298 \nL 220.988456 40.609977 \nL 224.062836 34.70578 \nL 227.137216 36.358811 \nL 230.211596 35.177971 \nL 233.285976 38.720489 \nL 236.360356 34.469685 \nL 239.434737 41.790816 \nL 242.509117 35.177971 \nL 245.583497 33.05275 \nL 248.657877 26.912096 \nL 251.732257 31.871911 \nL 254.806637 29.510232 \nL 257.881018 30.927167 \nL 260.955398 31.871911 \nL 264.029778 34.469685 \nL 267.104158 34.70578 \nL 270.178538 31.871911 \nL 273.252918 30.218518 \nL 276.327299 29.273774 \nL 279.401679 36.595268 \nL 282.476059 33.288845 \nL 285.550439 35.177971 \nL 288.624819 29.273774 \nL 291.699199 29.982423 \nL 294.77358 27.620744 \nL 297.84796 26.203809 \nL 300.92234 30.454614 \nL 303.99672 34.70578 \nL 307.0711 29.510232 \nL 310.14548 26.203809 \nL 313.219861 30.927167 \nL 316.294241 29.982423 \nL 319.368621 29.037679 \nL 322.443001 25.967714 \nL 325.517381 26.676 \nL 328.591761 25.02297 \nL 331.666142 27.620744 \nL 334.740522 26.203809 \nL 337.814902 21.244356 \nL 340.889282 26.439905 \nL 343.963662 25.02297 \nL 347.038042 27.620744 \nL 350.112423 25.967714 \nL 353.186803 28.801583 \nL 356.261183 29.982423 \nL 359.335563 25.259065 \nL 362.409943 25.02297 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pddd199b90c)\" d=\"M 58.046307 189.396099 \nL 61.120687 102.722273 \nL 64.195067 97.76282 \nL 67.269447 95.873332 \nL 70.343827 78.160741 \nL 73.418208 83.828843 \nL 76.492588 80.994611 \nL 79.566968 70.83961 \nL 82.641348 73.437384 \nL 85.715728 72.020449 \nL 88.790108 73.909575 \nL 91.864489 83.35629 \nL 94.938869 67.297092 \nL 98.013249 67.533187 \nL 101.087629 63.990669 \nL 104.162009 59.97596 \nL 107.236389 62.101181 \nL 110.31077 60.212055 \nL 113.38515 63.754574 \nL 116.45953 56.905633 \nL 119.53391 57.850377 \nL 122.60829 58.794759 \nL 125.68267 49.111948 \nL 128.757051 56.196984 \nL 131.831431 62.337639 \nL 134.905811 48.875853 \nL 137.980191 57.141728 \nL 141.054571 48.639757 \nL 144.128951 54.78005 \nL 147.203332 55.016145 \nL 150.277712 51.473627 \nL 153.352092 47.458918 \nL 156.426472 51.001436 \nL 159.500852 47.931109 \nL 162.575232 57.141728 \nL 165.649613 48.167204 \nL 168.723993 39.665233 \nL 171.798373 43.680304 \nL 174.872753 44.388591 \nL 177.947133 52.418371 \nL 181.021513 42.499465 \nL 184.095894 39.665233 \nL 187.170274 46.514174 \nL 190.244654 46.750269 \nL 193.319034 37.776107 \nL 196.393414 42.263007 \nL 199.467794 42.026912 \nL 202.542175 39.665233 \nL 205.616555 34.941876 \nL 208.690935 37.53965 \nL 211.765315 43.207751 \nL 214.839695 35.414429 \nL 217.914075 46.041983 \nL 220.988456 36.595268 \nL 224.062836 47.931109 \nL 227.137216 30.927167 \nL 230.211596 32.108006 \nL 233.285976 38.248298 \nL 236.360356 38.484394 \nL 239.434737 32.108006 \nL 242.509117 33.997132 \nL 245.583497 29.273774 \nL 248.657877 36.358811 \nL 251.732257 39.901329 \nL 254.806637 36.595268 \nL 257.881018 35.88662 \nL 260.955398 32.816292 \nL 264.029778 40.609977 \nL 267.104158 40.846073 \nL 270.178538 33.997132 \nL 273.252918 34.941876 \nL 276.327299 28.329393 \nL 279.401679 37.067459 \nL 282.476059 30.454614 \nL 285.550439 30.454614 \nL 288.624819 30.218518 \nL 291.699199 30.927167 \nL 294.77358 31.871911 \nL 297.84796 32.816292 \nL 300.92234 26.203809 \nL 303.99672 34.469685 \nL 307.0711 38.720489 \nL 310.14548 22.897387 \nL 313.219861 30.218518 \nL 316.294241 33.05275 \nL 319.368621 31.871911 \nL 322.443001 35.177971 \nL 325.517381 32.344102 \nL 328.591761 30.454614 \nL 331.666142 36.122715 \nL 334.740522 24.078226 \nL 337.814902 24.550417 \nL 340.889282 25.259065 \nL 343.963662 27.384649 \nL 347.038042 28.801583 \nL 350.112423 26.676 \nL 353.186803 35.650524 \nL 356.261183 27.384649 \nL 359.335563 33.524941 \nL 362.409943 22.188738 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#pddd199b90c)\" d=\"M 58.046307 159.874753 \nL 61.120687 113.822018 \nL 64.195067 101.541433 \nL 67.269447 88.788296 \nL 70.343827 94.456397 \nL 73.418208 102.722273 \nL 76.492588 99.179755 \nL 79.566968 92.803005 \nL 82.641348 83.828843 \nL 85.715728 84.537129 \nL 88.790108 83.592747 \nL 91.864489 79.341581 \nL 94.938869 77.215998 \nL 98.013249 87.135265 \nL 101.087629 80.758516 \nL 104.162009 76.035158 \nL 107.236389 76.507711 \nL 110.31077 78.632932 \nL 113.38515 71.311801 \nL 116.45953 71.547896 \nL 119.53391 64.46286 \nL 122.60829 79.577676 \nL 125.68267 72.965193 \nL 128.757051 80.050229 \nL 131.831431 67.060996 \nL 134.905811 70.130961 \nL 137.980191 64.699317 \nL 141.054571 71.075705 \nL 144.128951 66.588443 \nL 147.203332 64.46286 \nL 150.277712 66.824539 \nL 153.352092 74.618223 \nL 156.426472 69.894866 \nL 159.500852 57.141728 \nL 162.575232 64.46286 \nL 165.649613 63.518478 \nL 168.723993 56.905633 \nL 171.798373 57.377824 \nL 174.872753 61.392895 \nL 177.947133 68.241836 \nL 181.021513 63.045925 \nL 184.095894 58.794759 \nL 187.170274 55.724793 \nL 190.244654 51.709722 \nL 193.319034 56.43308 \nL 196.393414 62.573734 \nL 199.467794 63.282021 \nL 202.542175 55.25224 \nL 205.616555 72.49264 \nL 208.690935 55.960889 \nL 211.765315 53.127019 \nL 214.839695 51.94618 \nL 217.914075 57.850377 \nL 220.988456 61.62899 \nL 224.062836 51.709722 \nL 227.137216 57.613919 \nL 230.211596 59.031216 \nL 233.285976 45.097239 \nL 236.360356 48.639757 \nL 239.434737 48.167204 \nL 242.509117 51.94618 \nL 245.583497 52.890562 \nL 248.657877 50.765341 \nL 251.732257 55.016145 \nL 254.806637 55.25224 \nL 257.881018 51.709722 \nL 260.955398 55.960889 \nL 264.029778 58.086472 \nL 267.104158 53.363115 \nL 270.178538 55.488698 \nL 273.252918 53.363115 \nL 276.327299 48.875853 \nL 279.401679 44.152495 \nL 282.476059 55.25224 \nL 285.550439 46.750269 \nL 288.624819 51.473627 \nL 291.699199 48.167204 \nL 294.77358 56.905633 \nL 297.84796 51.001436 \nL 300.92234 49.584501 \nL 303.99672 47.222822 \nL 307.0711 45.097239 \nL 310.14548 44.861144 \nL 313.219861 50.292788 \nL 316.294241 43.443847 \nL 319.368621 42.971656 \nL 322.443001 44.861144 \nL 325.517381 35.177971 \nL 328.591761 51.001436 \nL 331.666142 52.418371 \nL 334.740522 51.709722 \nL 337.814902 53.59921 \nL 340.889282 42.263007 \nL 343.963662 55.960889 \nL 347.038042 42.971656 \nL 350.112423 49.111948 \nL 353.186803 45.097239 \nL 356.261183 44.388591 \nL 359.335563 48.167204 \nL 362.409943 48.167204 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#pddd199b90c)\" d=\"M 58.046307 174.753474 \nL 61.120687 99.41585 \nL 64.195067 91.149974 \nL 67.269447 86.662712 \nL 70.343827 79.341581 \nL 73.418208 81.231069 \nL 76.492588 80.286325 \nL 79.566968 65.171508 \nL 82.641348 68.950122 \nL 85.715728 68.477931 \nL 88.790108 62.573734 \nL 91.864489 55.25224 \nL 94.938869 59.503407 \nL 98.013249 58.086472 \nL 101.087629 58.794759 \nL 104.162009 56.43308 \nL 107.236389 59.267312 \nL 110.31077 53.835306 \nL 113.38515 43.207751 \nL 116.45953 51.709722 \nL 119.53391 53.363115 \nL 122.60829 49.348044 \nL 125.68267 46.750269 \nL 128.757051 47.695013 \nL 131.831431 51.473627 \nL 134.905811 52.890562 \nL 137.980191 41.082168 \nL 141.054571 46.278079 \nL 144.128951 44.861144 \nL 147.203332 42.73556 \nL 150.277712 40.373882 \nL 153.352092 40.609977 \nL 156.426472 39.193042 \nL 159.500852 38.484394 \nL 162.575232 36.831364 \nL 165.649613 44.624686 \nL 168.723993 40.846073 \nL 171.798373 31.635453 \nL 174.872753 32.816292 \nL 177.947133 31.163262 \nL 181.021513 35.414429 \nL 184.095894 41.318626 \nL 187.170274 30.927167 \nL 190.244654 33.524941 \nL 193.319034 29.746327 \nL 196.393414 38.248298 \nL 199.467794 32.580197 \nL 202.542175 29.746327 \nL 205.616555 37.303554 \nL 208.690935 31.871911 \nL 211.765315 32.344102 \nL 214.839695 29.982423 \nL 217.914075 32.580197 \nL 220.988456 37.303554 \nL 224.062836 34.469685 \nL 227.137216 32.816292 \nL 230.211596 32.580197 \nL 233.285976 30.218518 \nL 236.360356 30.927167 \nL 239.434737 33.997132 \nL 242.509117 32.580197 \nL 245.583497 31.399358 \nL 248.657877 28.801583 \nL 251.732257 30.691071 \nL 254.806637 26.439905 \nL 257.881018 27.148553 \nL 260.955398 26.439905 \nL 264.029778 31.163262 \nL 267.104158 28.329393 \nL 270.178538 30.454614 \nL 273.252918 29.273774 \nL 276.327299 25.259065 \nL 279.401679 30.218518 \nL 282.476059 30.691071 \nL 285.550439 21.244356 \nL 288.624819 28.092935 \nL 291.699199 24.550417 \nL 294.77358 19.118773 \nL 297.84796 26.439905 \nL 300.92234 29.510232 \nL 303.99672 22.897387 \nL 307.0711 23.842131 \nL 310.14548 24.314321 \nL 313.219861 28.092935 \nL 316.294241 23.842131 \nL 319.368621 25.731256 \nL 322.443001 21.007899 \nL 325.517381 27.384649 \nL 328.591761 29.982423 \nL 331.666142 24.550417 \nL 334.740522 27.148553 \nL 337.814902 27.148553 \nL 340.889282 25.02297 \nL 343.963662 23.133482 \nL 347.038042 27.85684 \nL 350.112423 25.259065 \nL 353.186803 23.606035 \nL 356.261183 24.550417 \nL 359.335563 17.937934 \nL 362.409943 23.133482 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pddd199b90c)\" d=\"M 58.046307 215.610661 \nL 61.120687 90.677784 \nL 64.195067 81.231069 \nL 67.269447 71.547896 \nL 70.343827 72.256545 \nL 73.418208 73.673479 \nL 76.492588 61.865086 \nL 79.566968 63.518478 \nL 82.641348 64.226764 \nL 85.715728 60.448151 \nL 88.790108 58.558663 \nL 91.864489 52.890562 \nL 94.938869 51.709722 \nL 98.013249 56.669537 \nL 101.087629 54.071401 \nL 104.162009 54.071401 \nL 107.236389 55.488698 \nL 110.31077 55.016145 \nL 113.38515 48.875853 \nL 116.45953 50.528883 \nL 119.53391 49.348044 \nL 122.60829 46.278079 \nL 125.68267 42.73556 \nL 128.757051 47.695013 \nL 131.831431 46.514174 \nL 134.905811 47.931109 \nL 137.980191 46.986365 \nL 141.054571 43.9164 \nL 144.128951 47.931109 \nL 147.203332 37.303554 \nL 150.277712 49.820597 \nL 153.352092 45.097239 \nL 156.426472 53.363115 \nL 159.500852 41.790816 \nL 162.575232 47.931109 \nL 165.649613 52.890562 \nL 168.723993 51.001436 \nL 171.798373 46.750269 \nL 174.872753 46.278079 \nL 177.947133 44.861144 \nL 181.021513 48.875853 \nL 184.095894 44.388591 \nL 187.170274 48.167204 \nL 190.244654 48.639757 \nL 193.319034 49.820597 \nL 196.393414 43.9164 \nL 199.467794 48.875853 \nL 202.542175 43.680304 \nL 205.616555 40.137786 \nL 208.690935 46.986365 \nL 211.765315 42.971656 \nL 214.839695 39.665233 \nL 217.914075 57.377824 \nL 220.988456 43.680304 \nL 224.062836 40.137786 \nL 227.137216 44.624686 \nL 230.211596 38.248298 \nL 233.285976 44.152495 \nL 236.360356 46.278079 \nL 239.434737 36.595268 \nL 242.509117 49.348044 \nL 245.583497 44.388591 \nL 248.657877 40.609977 \nL 251.732257 38.956947 \nL 254.806637 37.53965 \nL 257.881018 38.484394 \nL 260.955398 41.790816 \nL 264.029778 36.595268 \nL 267.104158 41.318626 \nL 270.178538 47.931109 \nL 273.252918 42.263007 \nL 276.327299 39.193042 \nL 279.401679 43.207751 \nL 282.476059 37.303554 \nL 285.550439 32.108006 \nL 288.624819 39.665233 \nL 291.699199 33.05275 \nL 294.77358 42.026912 \nL 297.84796 41.318626 \nL 300.92234 42.026912 \nL 303.99672 41.318626 \nL 307.0711 40.373882 \nL 310.14548 36.122715 \nL 313.219861 52.418371 \nL 316.294241 34.469685 \nL 319.368621 39.901329 \nL 322.443001 37.776107 \nL 325.517381 31.163262 \nL 328.591761 35.88662 \nL 331.666142 30.218518 \nL 334.740522 36.358811 \nL 337.814902 45.56943 \nL 340.889282 39.665233 \nL 343.963662 38.248298 \nL 347.038042 35.650524 \nL 350.112423 30.218518 \nL 353.186803 33.288845 \nL 356.261183 34.70578 \nL 359.335563 35.414429 \nL 362.409943 37.776107 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 42.828125 225.494297 \nL 42.828125 8.054297 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 377.628125 225.494297 \nL 377.628125 8.054297 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 42.828125 225.494297 \nL 377.628125 225.494297 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 42.828125 8.054297 \nL 377.628125 8.054297 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 246.623438 220.494297 \nL 370.628125 220.494297 \nQ 372.628125 220.494297 372.628125 218.494297 \nL 372.628125 144.713047 \nQ 372.628125 142.713047 370.628125 142.713047 \nL 246.623438 142.713047 \nQ 244.623438 142.713047 244.623438 144.713047 \nL 244.623438 218.494297 \nQ 244.623438 220.494297 246.623438 220.494297 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 248.623438 150.811485 \nL 268.623438 150.811485 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_15\">\n     <!-- accuracy_baseline -->\n     <g transform=\"translate(276.623438 154.311485)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"564.648438\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"625.927734\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"678.027344\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"739.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"767.333984\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"795.117188\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"858.496094\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 248.623438 165.767735 \nL 268.623438 165.767735 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_16\">\n     <!-- accuracy_A1 -->\n     <g transform=\"translate(276.623438 169.267735)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-49\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 248.623438 180.723985 \nL 268.623438 180.723985 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_17\">\n     <!-- accuracy_A2 -->\n     <g transform=\"translate(276.623438 184.223985)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-50\"/>\n     </g>\n    </g>\n    <g id=\"line2d_26\">\n     <path d=\"M 248.623438 195.680235 \nL 268.623438 195.680235 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_27\"/>\n    <g id=\"text_18\">\n     <!-- accuracy_A3 -->\n     <g transform=\"translate(276.623438 199.180235)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-51\"/>\n     </g>\n    </g>\n    <g id=\"line2d_28\">\n     <path d=\"M 248.623438 210.636485 \nL 268.623438 210.636485 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_29\"/>\n    <g id=\"text_19\">\n     <!-- accuracy_A4 -->\n     <g transform=\"translate(276.623438 214.136485)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n      <use x=\"451.171875\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"501.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"569.580078\" xlink:href=\"#DejaVuSans-52\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pddd199b90c\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"42.828125\" y=\"8.054297\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "print(\"Original Model Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_baseline, model_accuracy_baseline = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_baseline}, Accuracy: {model_accuracy_baseline}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Model Results\n",
      "268/268 - 0s - loss: 0.5611 - accuracy: 0.7307\n",
      "Loss: 0.561127245426178, Accuracy: 0.7307288646697998\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "print(\"Alternative Model 1 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A1, model_accuracy_A1 = nn_A1.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A1}, Accuracy: {model_accuracy_A1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 1 Results\n",
      "268/268 - 0s - loss: 0.5581 - accuracy: 0.7294\n",
      "Loss: 0.5580662488937378, Accuracy: 0.7294460535049438\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "print(\"Alternative Model 2 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A2, model_accuracy_A2 = nn_A2.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A2}, Accuracy: {model_accuracy_A2}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 2 Results\n",
      "268/268 - 0s - loss: 0.5651 - accuracy: 0.7255\n",
      "Loss: 0.5651211738586426, Accuracy: 0.7254810333251953\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "print(\"Alternative Model 3 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A3, model_accuracy_A3 = nn_A3.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A3}, Accuracy: {model_accuracy_A3}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 3 Results\n",
      "268/268 - 0s - loss: 0.5734 - accuracy: 0.7314\n",
      "Loss: 0.5733740925788879, Accuracy: 0.7314285635948181\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "print(\"Alternative Model 4 Results\")\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss_A4, model_accuracy_A4 = nn_A4.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the model loss and accuracy results\n",
    "print(f\"Loss: {model_loss_A4}, Accuracy: {model_accuracy_A4}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alternative Model 4 Results\n",
      "268/268 - 0s - loss: 0.5711 - accuracy: 0.7305\n",
      "Loss: 0.5711314678192139, Accuracy: 0.7304956316947937\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Save each of your alternative models as an HDF5 file.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A1.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A1.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A2.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A2.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A3.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A3.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "# Set the file path for the first alternative model\n",
    "file_path = Path(\"./Resources/AlphabetSoup_A4.h5\")\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn_A4.save(file_path)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('dev': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "930eee74a1f2acacbb765ecf4f41bca31b45440fd68d4dff19855f34b00f2967"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}